\chapter{مقدمه}

عمومیت پیدا کردن داده‌های حجیم مانند داده‌های حجیم تحت وب و جریان‌ داده‌های بزرگ در کاربردهای جدید،
موجب به وجود آمدن فرصت‌های و چالش‌هایی برای مهندسین و دانشمندان شده است.%
\cite{li2007stable}
برای مثال، زمانی که ماتریس داده 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
ابعادی در حد وب داشته باشد، عملیات ساده‌ای مانند محاسبه
$\mathbf{A} \mathbf{A}^T$
سخت می‌شود.
برای ارائه و نگهداری داده‌های حجیم در حافظه‌ای کوچک و برای استخراج اطلاعات آماری اصلی از مجوعه‌ای از بیانی محدود، روش‌های گوناگونی نمونه‌برداری توسعه‌ یافته است. به طور کلی روش 
\textit{نگاشت تصادفی پایدار}%
\LTRfootnote{stable random projection
\label{ft:stp}}
 برای داده‌های با دم سنگین خیلی خوب کار می‌کند.

روش نگاشت تصادفی پایدار، ماتریس داده‌های اولیه 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
را در ماتریس تصادفی 
$\mathbf{R} \in \mathbb{R}^{D \times k} (k \ll D)$
 ضرب می‌کند و نتیجه ماتریس
$\mathbf{B} = \mathbf{AR} \in \mathbb{R}^{n \times k}$
است. 
معمولا درایه‌های ماتریس تصادفی
$\mathbf{R}$
به صورت 
\lr{i.i.d}%
\LTRfootnote{independent and identically distributed random variables}
از یک توزیع 
$\alpha$
-پایدار متقارن انتخاب می‌شوند (
$ 0 < \alpha \leq 2$
).
ما می‌توانیم مشخصه‌های 
$l_\alpha$
را در 
$\mathbf{A}$
بر اساس 
$\mathbf{B}$
تخمین بزنیم. در مورد حالت 
$l_2$
مزیت توزیع تصادفی پایدار توسط لم 
\lr{JL}%
\LTRfootnote{Johnson-Lindenstrauss}
برجسته شده است. لم 
\lr{JL}
بیان می‌دارد که کافی است 
$k=O(\frac{ \log  n}{\epsilon^ 2})$
باشد تا هم فاصله دو به دویی با نرم 
$l_\alpha$
در 
$\mathbf{A}$
را بتوان با ضریب 
$1 \pm \epsilon$
از روی ماتریس 
$\mathbf{B}$
تخمین زد. در رساله پینگ لی% 
\LTRfootnote{Ping Li}
\cite{li2007stable}
لمی مشابه لم 
\lr{JL}
برای 
$0 < \alpha < 2$
اثبات شده است. روش نگاشت تصادفی پایدار به یک مسئله تخمین آماری کاهش می‌یابد برای تخمین پارامتر مقیاس برای یک توضیع پایدار $\alpha$ متقارن. این مسئله از این جهت مورد توجه قرار می‌گیرد زیرا ما به دنبال برآوردی می‌گردیم که هم از نظر آماری درست باشند و هم از نظر محاسباتی مقرون به صرفه. برآوردگرهای مختلفی را مطالعه و مقایسه کردیم. شامل میانگین حسابی، میانگین هندسی، میانگین هارمونیک، تقسیم توانی%
\LTRfootnote{fractional power}
و برآوردگر حداکثر بزرگنمایی.

در این پایان‌نامه ما به بررسی موارد خاصی از نگاشت تصادفی پایدار می‌پردازیم. برای نرم 
$l_2$
ارتقایی را با استفاده از اطلاعات حاشیه‌ای پیشنهاد می‌کنیم. همچنین برای حالت $l_2$ می‌توان ماتریس نگاشت‌گر را از یک توزیع زیرگاوسی%
\LTRfootnote{sub-Gaussian}
بسیار کوچکتر به جای توزیع نرمال انتخاب کرد. با در نظر گرفتن محدودیت‌های معقولی می‌توان، از یک توزیع خاص زیر گاوسی استفاده کرد. این توزیع شامل 
$[-1,0,1]$
با احتمالات 
$\{ \frac{1}{2s}, 1-\frac{1}{s}, \frac{1}{2s} \}$
با مقادیر بسیار بزرگی برای 
‌‌‌$s$
(به عبارتی، نگاشت تصادفی خیلی تُنُک %
\LTRfootnote{very sparse random projections}
) می‌تواند به خوبی نگاشت تصادفی نرمال عمل کند. برای حالت نرم 
$l_1$
به عبارتی دیگر نگاشت تصادفی کوشی %
\LTRfootnote{Cauchy random projection}
انجام تخمین کاری نسبتا جذاب است. برای مثال، محاسبه برآوردگر بیشینه درستنمایی 
\lr{MLE}
در این حالت از لحاظ محاسباتی ممکن است. و یک توزیع معکوس گاوسی %
\LTRfootnote{inverse Gaussian}
برای مدل‌سازی دقیق توزیع 
\lr{MLE}
بیان شده است.

روش نگاشت تصادفی از پراکندگی داده‌ها استفاده‌ای نمی‌کند. در حالی که داده‌های بزرگ مقیاس معمولا بسیار پراکنده هستند. از روش نگاشت تصادفی می‌توان برای حل مسائل بزرگ مقیاس در علوم و مهندسی در موتورهای جستجو و سیستم‌های اخذ داده، پایگاه‌های داده، سیستم‌های جریان داده جدید، جبر خطی عددی و بسیاری از کارهای یادگیری ماشین و داده کاوی که شامل محاسبه حجیم فاصله‌ها است، استفاده کرد.

در فصل بعد مروری خواهیم داشت بر ادبیات مسئله. در ابتدا کاهش بعد و روش‌های مرسوم برای آن را معرفی می‌کنیم. سپس به خوشه‌بندی روش‌های آن خواهیم پرداخت و در ادامه شاخص‌هایی را معرفی خواهیم کرد که کارامدی خوشه‌بندی را مورد بررسی قرار می‌دهند. در ادامه همین فصل موضوع داده‌های حجیم و اهمیت آن را با معرفی مثال‌هایی شرح خواهیم داد. نگاشت تصادفی را به عنوان راه حلی برای مسئله‌ی نمونه‌گیری از داده‌های حجیم بررسی می‌کنیم و در نهایت به کاربردهای آن خواهیم پرداخت. در فصل سوم مسئله‌ی نگاشت تصادفی پایدار و انواع آن را بررسی می‌کنیم و مبانی این روش مزایا و معایب هر یک از انواع آن را شرح خواهیم داد. در فصل چهارم نحوه‌ی پیاده‌سازی و داده‌های مورد استفاده را شرح می‌دهیم. در فصل پنج هم نتایج مقایسه‌ای را مطرح کرده و به طبقه‌بندی کارهای بعدی ممکن می‌پردازیم.







