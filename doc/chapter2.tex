\chapter{
کاهش بعد و داده‌های بزرگ مقیاس
}


%========================================Section====================================
\section{
کاهش بعد
}
\subsection{
تحلیل مولفه‌های اصلی
}

تحلیل چند متغیره معمولا بر روی داده‌هایی که شامل تعداد زیادی از متغیرهای مرتبط با هم هستند انجام می‌شود.\\
روش تحلیل مؤلفه‌های اصلی(
\lr{PCA}
)%
\LTRfootnote{Principal Components Analysis}
 یک ابزار کاهش بعد است که می توان از آن برای کاهش یک مجموعه‌ی بزرگ از متغیرها به مجموعه‌ی کوچکتری که غالب اطلاعات مجموعه‌ی بزرگ را دارد استفاده کرد.

روش تحلیل مولفه‌های اصلی یک تابع ریاضی است که تعدادی متغیر (احتمالا) همبسته را به تعداد (کمتر یا مساوی) متغیرهای غیرهمبسته به نام «مولفه‌های اصلی» تبدیل می‌کند.

بیشترین میزان اطلاعات ممکن در داده در اولین مولفه اصلی ثبت می‌شود. بیشترین میزان اطلاعات ممکن باقیمانده به ترتیب در مولفه‌های بعدی ثبت می‌شوند.

تحلیل مولفه‌های اصلی مشابه یک تابع چند متغیره دیگر به نام تحلیل عاملی است. این دو روش در موارد زیادی با یکدیگر اشتباه گرفته می‌شوند، و تفاوت بین این دو، و انواع تحلیل هایی که هر یک برایشان مناسب تر هستند به درستی تشخیص داده نمی‌شود.
به طور سنتی، تحلیل مولفه های اصلی بر روی ماتریس های متقارن مربعی انجام می شود. این ماتریس‌ها می تواند یکی از انواع
\lr{SSCP}%
\LTRfootnote{pure sums of squares and cross products}
 (مجموع خالص مربعات و ضرب های داخلی)، ماتریس کوواریانس%
\LTRfootnote{covariance}
 (مجموع مقیاس شده مربعات و ضرب های داخلی)، یا ماتریس همبستگی%
\LTRfootnote{correlation}
 (مجموع مربعات و ضرب های داخلی داده های استاندارد شده) باشد.

نتایج تحلیل روی ماتریس‌های از نوع
\lr{SSCP}
 و کوواریانس تغییری ندارند، چرا که تغییرات آنها فقط در یک ضریب مقیاس قابل مشاهده است.

از ماتریس‌های همبستگی زمانی استفاده می‌شود که واریانس متغیرهای منحصر به فرد تفاوت‌های زیادی داشته باشد، ‌و یا واحد‌های اندازه‌گیری این متغیرها متفاوت باشد.

\subsubsection{
اهداف نحلیل مولفه‌های اصلی
}

تحلیل مولفه‌های اصلی فضای مشخصه‌ها را از تعداد زیادی متغیر به تعداد کمتری عامل کاهش می‌دهد، و یک تابع «غیر وابسته» است (یعنی نیازی وجود ندارد که یک متغیر وابسته تعیین شده باشد).

تحلیل مولفه های اصلی یک روش کاهش یا فشرده سازی ابعاد است. هدف، کاهش بعد است و تضمینی وجود ندارد که این ابعاد قابل تفسیر باشند.

در نهایت، انتظار آن است که زیرمجموعه‌ای از متغیرها از یک مجموعه بزرگتر انتخاب شود، به گونه‌ای که متغیرهای اولیه بیشترین همبستگی را با مولفه اصلی داشته باشند.

تحلیل مولفه‌های اصلی به دنبال رسیدن به ترکیبی خطی از متغیرها است، به گونه ای که بیشینه واریانس از آنها قابل استخراج باشد. پس از آن، این واریانس حذف شده و ترکیب خطی دومی جستجو می‌شود که بیشینه باقی‌مانده واریانس را توصیف می‌کند، و این روند ادامه پیدا می کند. به این روش، روش محور اصلی گفته می شود و عامل های متعامد غیرهمبسته را به دست می‌دهد. تحلیل مولفه‌های اصلی، واریانس (مشترک و یکتای) کل را شرح می‌دهند.

\textbf{
ویژه بردارها:
} مولفه‌های اصلی، هر دو واریانس مشترک و یکتای متغیرها را منعکس می‌کنند و بنابراین ممکن است که این روش به عنوان یک روش واریانس محور دیده شود که هم به دنبال بازتولید واریانس متغیر کل با تمام مولفه‌ها و هم بازتولید همبستگی‌ها است.
مولفه های اصلی، ترکیب های خطی از متغیرهای اولیه هستند که بر اساس میزان سهمشان در به وجود آمدن واریانس در یک بعد متعامد مشخص، وزن دهی می‌شوند. وزن‌های داده شده برای هر یک از مولفه‌های اصلی نسبت به داده‌های اولیه، ویژه بردارها هستند.

\textbf{
ویژه مقدارها:
}
 ویژه مقدار یک مولفه، واریانس همه‌ی متغیرهایی را که به آن عامل مرتبط هستند اندازه گیری می کند.
نسبت ویژه مقدارها، نسبت اهمیت توصیفی عامل ها با توجه به متغیرها است. اگر یک عامل دارای ویژه مقدار پایین باشد، نشانگر آن است که اثر کمی روی توصیف واریانس در متغیرها دارد، و ممکن است از آن در مقابل عامل های مهمتر چشم پوشی شود.
در ویژه مقدارها میزان تغییر در نمونه کل حساب شده است.

ویژه مقدار یک عامل ممکن است حاصل جمع مربعات عامل های تمامی متغیرها باشد. باید توجه شود که ویژه مقدارهای مرتبط با راه حل های دورانی و غیردورانی متفاوت خواهند بود، اگرچه مقدار کل آنها یکسان است.

برای به دست آوردن واریانس همه متغیرها که توسط عامل لحاظ می شود، مجموع مربعات بارگذاری های عامل برای آن عامل (ستون) را جمع کرده، و بر تعداد متغیرها تقسیم می کنیم. (توجه کنید که تعداد متغیرها برابر با مجموع واریانس آنهاست، چرا که واریانس یک متغیر استاندارد شده مساوی با ۱ است). این کار مشابه تقسیم ویژه مقدار عامل بر تعداد متغیرها است.

امتیاز 
\lr{PC}
: این امتیازها، امتیازهای هر نمونه (ردیف) در هر عامل (ستون) هستند. امتیاز عامل برای یک نمونه و برای یک عامل داده شده، به صورت مجموع حاصل ضرب  امتیاز استاندارد نمونه در هر متغیر با بارگذاری عامل مربوطه برای عامل داده شده محاسبه می‌شود.
$\blacksquare$

%=======================================Section====================================
\section{
خوشه‌بندی و شاخص‌های سنجش
‌}
\subsection{
شاخص رند تعدیل شده
}

برای مقایسه نتایج خوشه‌بندی در کنار شاخص‌های بیرونی، نیازمند معیارهای مورد توافق هستیم.
برای مجموعه‌ی 
$n$
 عضوی 
$S = \{O_1, \ldots, O_n\}$
، فرض کنید که
$U = \{u_1, \ldots, u_R\}$
 و
$V = \{v_1, \ldots, v_R\}$
 نمایانگر دو افراز متفاوت از عضوهای $S$ برای 
% TODO
و 
% TODO 
 هستند ‌به گونه ای که 
% TODO
 و 
% TODO
. فرض کنید که یک $U$ شاخص خارجی و $V$ یک نتیجه خوشه‌بندی است. فرض کنید $a$ تعداد جفت عضوهایی باشد که در یک کلاس یکسان $U$ و خوشه یکسان $V$ هستند، $b$ تعداد جفت عضوهایی باشد که در یک کلاس یکسان $U$ اما خوشه متفاوت $V$ هستند، $c$ تعداد جفت عضوهایی باشد که در یک کلاس متفاوت $U$ و خوشه یکسان $V$ هستند، و $d$ تعداد جفت عضوهایی باشد که در یک کلاس متفاوت $U$ و خوشه متفاوت $V$ هستند. می‌توان مقادیر $a$ و $d$ را به عنوان موارد توافق، و مقادیر $b$ و $c$ را به عنوان موارد عدم توافق تعریف کرد. شاخص رند [رند،‌۱۹۷۱]
% TODO reference and footnote
 به صورت 
% TODO
 تعریف می‌شود. شاخص رند عددی بین ۰ و ۱ است. زمانی که دو خوشه کاملا در حالت توافق باشند، مقدار شاخص رند برابر با ۱ خواهد بود.

یکی از مشکلات شاخص رند آن است که مقدار مورد انتظار شاخص رند دو خوشه تصادفی یک مقدار ثابت (به عنوان مثال صفر) نیست. مقدار شاخص رند تعدیل شده پیشنهادی توسط [هوبرت و اربی، ۱۹۸۵]
% TODO reference and footnote
 بر پایه این فرض است که توزیع ابرهندسی تعمیم یافته برای مدل تصادفی استفاده می‌شود، به عبارت دیگر خوشه های $U$ و $V$ به شکلی انتخاب می‌شوند که تعداد عضوهای کلاس‌ها و خوشه‌ها ثابت باشد. فرض کنید 
$n_{ij}$
 تعداد عضوهایی باشد که هم در کلاس 
$u_i$
 و هم در خوشه 
$v_j$
 هستند. و فرض کنید 
$n_{i.}$
 و 
$n_{.j}$ 


%========================================Section===================================
\section{داده‌های حجیم}
عبارات زیر از سایت 
\lr{\textit{Information Week}}
نقل قول شده‌اند%
\LTRfootnote{http://www.informationweek.com/news/showArticle.jhtml?articleID=175801775}
:

\begin{itemize}
\item
مقدار داده‌ای که توسط کسب و کارها ذخیره می‌شود تقریبا هر ۱۲ تا ۱۸ ماه دو برابر می‌شود.
\item
پایگاه داده‌ها بیشتر هم ‌زمان شده‌اند. فروشگاه‌های زنجیره‌ای 
\lr{Wall-Marat}
داده‌های فروش را هر ساعت به روز می‌کند.
\item
اضافه شدن یک میلیون خط داده اجازه جستجوهای پیچیده‌تری را می‌دهد. شرکت 
\lr{EBay}
به کارمندان اجازه می‌دهد برای بدست آوردن درکی عمیق‌تر در خصوص رفتار مشتریان در میان داده‌های حراج در بازه‌های زمانی کوتاه جستجو کنند.
\item
بزرگترین پایگاه داده‌ها توسط، مرکز شتابدهنده خطی استاندارد، مرکز تحقیقات ناسا، آژانس امنیت ملی و ... در ابعادی در محدوده‌ی پتابایت (هزار ترابایت 
$10^15$
بایت)، اداره می‌شوند.
\end{itemize}

پدیده نو ظهور مجموعه‌ داده‌ّای حجیم، چالش‌های محاسباتی در بسیاری کاربردهای علمی و تجاری به وجود آورده است. شامل اخترفیزیک، بیوتکنولوزی، جمعیت شناسی%
\LTRfootnote{demographics}
، مالی، سیستم‌های اطلاعات جغرافیایی، دولت، دارو، ارتباطات از راه دور، محیط زیست و اینترنت.

\subsection{داده‌های حجیم وب}

وب چقدر بزرگ است؟  
\autoref{tab:searchengin}
نشان‌دهنده تعداد بازدید صفحات در موتورهای جستجوی امروزی است. به طور تخمینی حدود 
$D = 10^10$
صفحه‌ی وب را می‌توان بر اساس بازدید دو واژه‌ی بسیار پر کاربرد «
\lr{A}
» و «
\lr{THE}
» تخمین زد. 
\autoref{tab:searchengin}
 همچنین نشان می‌دهد که حتی کلماتی که به ندرت کاربرد دارند هم تعداد زیادی بازدید دارند.

\begin{table}[h]
\centering
\begin{latin}
\begin{tabular}{lll}
\hline
Query        & Google         & Bing        \\ \hhline{===}
A            & 25,270,000,000 & 175,000,000 \\
The          & 25,270,000,000 & 101,000,000 \\
Kalevala     & 7,440,000      & 939,000     \\
Griseofulvin & 1,163,000      & 332,000     \\
Saccade      & 1,030,000      & 388,000     \\
\hline
\end{tabular}
\end{latin}
\caption{
تعداد بازدید صفحات برای کلمات با بازخورد بالا و کلمات با بازخورد نادر
}
\label{tab:searchengin}
\end{table}


کلماتی با بازخورد معمولی چه میزان بازدید دارند؟ برای جواب این سوال ما به طور تصادفی ۱۵ صفحه از لغتنامه‌‌ی آموزشی انتخاب می‌کنیم.
\cite{litez94}
(لغتنامه‌ای با ۵۷،۱۰۰ کلمه) و اولین کلمه در هر صفحه را مد نظر قرار می‌دهیم. میانه‌ی آماری بر اساس جستجو‌گر گوگل ۱۰ میلیون صفحه برای کلمه است.

زبان انگلیسی چند کلمه دارد؟ در اینجا عبارتی را از 
\lr{AskOxford.com}
نقل قول می‌کنیم:

« این بیان میدارد که حداقل یک چهارم میلیون واژه‌ی انگلیسی مستقل وجود دارد. به جز افعال صرفی و کلمات فنی و ناحیه‌ای که توسط 
\lr{OED}%
\LTRfootnote{Oxford English Dictionary}
تحت پوشش قرار نمی‌گیرند یا کلماتی که هنوز به لغتنامه‌های منتشر شده اضافه نشده‌اند. در صورتی که این موارد هم در نظر گرفته شوند تعداد لغات در حدود سه چهارم میلیون لغت خواهد بود »

بنابراین اگر یک ماتریس «عبارت به سند» 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
در نظر بگیریم. در ابعاد وب این ماتریس در ابعاد 
$n \approx 10^6$
و 
$D \approx 10^10$
بزرگ خواهد شد.
در اینجا عدد 
$(i,j)$
در 
$\mathbf{A}$
تعداد ظهور واژه 
$i$
در سند 
$j$
را نشان می‌دهد.

کارکردن با ماترسی در این ابعاد بزرگ چالش برانگیز است. برای مثال، شاخص 
\lr{LSI}%
\LTRfootnote{latent semantic indexing}
\cite{litez58}
و یک مدل موضوعی فراگیر، از 
\lr{SVD}%
\LTRfootnote{singular value decomposition}
بر روی ماتریس عبارت به سند استفاده می‌کند. که انجام این عملیات در ابعاد وب قطعا غیرممکن است.

یک مشکل اصلی در قبال مجموعه داده‌های سنگین، حافظه کامپیوتر است. به این دلیل که ابعاد و سرعت حافظه فیزیکی بسیار رشد کمتری در مقایسه با پردازنده‌ها (
\lr{CPU}
) دارد. این پدیده به عنوان دیوار حافظه شناخته می‌شود
\cite{litez139, litez168}
. برای مثال،‌ هر چند ممکن است تمامی رخداد‌های همزمان دوتایی از پیش محاسبه شوند، ولی نگهداری این حجم از داده در حافظه غیر ممکن است. علاوه بر این، گاهی اوقات تخصیص‌‌هایی با بیش از دو عامل هم اهمیت پیدا می‌کنند زیرا درخواست‌ها ممکن است شامل بیش از دو واژه هم باشند. یک راه حل ممکن این است که یک «نمونه» از 
$\mathbf{A}$
 نگهداری شود و همزمانی‌ها بر اساس این نمونه در حین کار تخمین زده شوند. ما حدس می‌زنیم که این روش توسط موتورهای جستجوی امروزی مورد استفاده قرار می‌گیرد، هر چند که روش واقعی قطعا جزو اسرار تجاری آن‌ها است.

هر چند که انتظار می‌رود تخمین‌ها سازگار باشند و فرکانس‌های جفت شده باید با افزایش عبارت به درخواست، کاهش پیدا کنند. 
\autoref{tab:querylengthcon}
نشان می‌دهد که تخمین‌های بیان شده با موتورهای جستجوی فعلی، همیشه سازگار نیستند.
\begin{table}[h]
\centering
\begin{latin}
\begin{tabular}{lll}
\hhline{===}
Query        				& Hits(Bing)    & Hits(Google) 	\\ \hline
America            			& 150,731,182 	& 393,000,000 	\\
America \& China          		& 15,240,116  	& 66,000,000 	\\
America \& China \& Britain     	& 235,111     	& 6,090,000     \\
America \& CHina \& Britain \& Japan 	& 154,444     	& 23,300,000    \\
\hhline{===}
\end{tabular}
\end{latin}
\caption{
با افزایش تعداد عبارات در درخواست، باید فرکانس‌های جفت شده کاهش پیدا کنند. ولی تخمین‌های بیان شده توسط موتورهای جستجو گاهی این موضوع تثبیت شده را نقض می‌کنند.
}
\label{tab:querylengthcon}
\end{table}

با اینکه، تعداد کل واژه‌های انگلیسی ( که به‌طور صحیح نوشته شده‌اند) هم اکنون شگفت‌اور است، در بسیاری کاربردهای متن کاوی، ما باید با ابعاد بسیار بزرگتری سر و کار داشته باشیم. در حالی که یک سند ممکن است بیانگر برداری از تک واژه‌ها باشد (به عبارت دیگر، مدل کیسه لغات%%
\LTRfootnote{bag-of-words}
). معمولا بهتر است سند به عنوانن یک بردار از لغات به صورت 
\lr{l}
پیوسته %
\LTRfootnote{l-shingles}
\cite{litez34}
بیان شود. برای مثال، با استفاده از مدل ۳ پیوسته، جمله‌ی
\lr{"It is a nice day"}
به مجموعه‌ی زیر تجزیه می‌شود. 
\lr{\{"it is a", "is a nice", "a nice day"\}}
این مدل به طور جشمگیری ابعداد داده‌ها را افزایش می‌دهد. به خاطر اینکه، اگر مجموعه‌ی 
$10^6$
تک لغت انگلیسی موجود داشته باشد. مدل ۳ پیوسته تعداد ابعاد را از 
$10^6$
به 
$10^{18}$
افزایش می‌دهد.

\subsection{
جریان‌های داده‌ی حجیم
}
در بسیاری کاربردهای جدید پردازش داده، جریان‌های داده‌ی حجیم نقش بنیادی دارند. جریان‌های داده‌ای که از روترهای اینترنت، سوئیچ‌های تلفن، رصد امسفر، شبکه‌های سنسور، شرایط ترافیکی بزرگراهی، داده‌های مالی و غیره 
\cite{litez5, litez141, litez49, litez19, litez96, litez69, litez91}
حاصل می‌شوند.

برخلاف پایگاه‌ داده‌های سنتی، معمول نیست که جریان‌های داده‌ی حجیم (که با سرعت زیادی منتقل می‌شوند) در جای نگهداری شوند. بنابراین پردازش معمولا به طور همزمان انجام می‌شوند. برای مثال، گاهی اوقات «رصد تصویری» داده‌ها با رصد تغییرات زمانی برخی آماره‌ها کفایت می‌کند. برای مثال آماره‌های نظیر: مجموع، تعداد آیتم‌های مجزا، برخی نرم‌های 
$l_\alpha$
. در برخی کاربردها (برای مثال، طبقه‌بندی صدا/محتوا و جدا سازی) نیاز است یک مدل یادگیری آماری برای کلاسه‌بندی%
\LTRfootnote{Classification}
یا خوشه‌بندی%
\LTRfootnote{Clustering}
جریان داده‌های حجیم توین شود. ولی معمولا فقط می‌توانیم یک‌بار داده‌ها را مورد بررسی قرار دهیم.

یک خاصیت مهم جریان‌های داده‌ای این است که دینامیک هستند. به عنوان یک مدل محبوب، جریان 
$u$
شامل ورودی‌های 
$(i, u_i)$
است که 
$i = 1 to D$
. برای مثال،
$D = 2^{64}$
زمانی که جریان بیان‌گر 
\lr{IP}
آدرس‌ها است.
\footnote{
هرچند ما بیشتر اوقات تعداد دقیق ابعاد (
\lr{D}
) یک جریان داده را نمی‌دانیم ولی در بیشتر کاربردها کافی است حد بالایی محافظه‌کارانه‌ای را در نظر بگیریم. برای مثال 
$D = 2^{64}$
زمانی که جریان بیانگر 
\lr{IP}
های ورودی است. همچنین این یکی از دلایلی است که داده‌ها بسیار پراکنده هستند. به این نکته توجه داشته باشید که ابعاد بسیار بزرک تاثیری در محاسبه‌ی فاصله‌ها و نمونه‌گیری طی الگوریتم‌های معرفی شده در این پایان‌نامه ندارد.
}
ورودی‌ها ممکن است به هر ترتیبی باشند و ممکن است مرتبا به روز شوند. ذات دینامیک جریان داده‌های حجیم فرآیند نمونه‌گیری را بسیار چالش‌برانگیزتر ا زمانی ‌می‌کند که با داده‌های ایستا سر و کار داریم.

%========================================Section===================================
\section{
چالش‌های نمونه‌گیری از داده‌های حجیم
}
در حالی که مسائل جذاب و چالش‌برانگیزی با ورود داده‌های حجیم شکل گرفته‌اند، این پایان‌نامه بر روی توسعه‌ی روش‌های نمونه‌گیری برای محاسبه فاصله در داده‌هایی با ابعاد بسیار بالا با استفاده از حافظه محدود تمرکز دارد.

در کاربردهای مدل‌سازی آماری و یادگیری ماشین، در اغلب موارد به جای داده‌های اصلی به فاصله، به خوصو فاصله‌ی جفتی نیاز داریم. برای مثال، محاسبه ماتریس گرام%
\LTRfootnote{Gram matrix}
$\mathbf{AA^T}$
در آمار و یادگیری ماشین معمول است. 
$\mathbf{AA^T}$
بیانگر همه‌ی ضرب‌های داخلی دوتایی در ماتریس داده‌ی 
‌$\mathbf{A}$
است.

دو داده‌ی 
$u_1, u_2 \in \mathbb{R}^D$
داده شده‌اند. ضرب داخلی آن‌ها ( که با 
$a$
نمایش داده می‌شود) و 
$l_\alpha$
(که با 
$d_{(\alpha)}$
نمایش داده می‌شوند با عبارات زیر تعریف می‌شوند:
\footnote{
ما فاصله $l_\alpha$ را به صورت 
$d_{(\alpha)} = \sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^\alpha$
تعریف کرده‌ایم. به جای اینکه به شکل
$\mathopen( \sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^\alpha \mathclose)^{1/\alpha}$
تعریف کنیم. زیرا شکل اول در کاربردهای عملی عمومیت بیشتری دارد. برای مثال، لم 
\lr{JL}
، در ادبیات معمولا به شکل توان دو $l_2$ بیان می‌شود. 
$\sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^2$
به جای 
$\mathopen( \sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^2 \mathclose)^{1/2}$
. در این پایان‌نامه، ما برای سادگی 
$\sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^2$
را «فاصله $l_2$» بیان می‌کنیم به جای «مربع فاصله‌ی $l_2$».
}
\begin{align}
a=u_1^T u_2 = \sum_{i=1}^{D} u_{1,i} u_{2,i} \\
d_{(\alpha)} = \sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^\alpha 
\label{eq:1hP}
\end{align}

به این نکته توجه داشته باشید که هم ضرب داخلی و هم فاصله به شکل جمع
$D$
جمله تعریف می‌شوند. بنابراین، زمانی که داده‌ها به اندازه‌ای حجیم هستند که نمی‌توان به طور کارا آن‌ها را مدیریت کرد، نمونه‌گیری خیلی عادی به نظر می‌رسد تا بتوان با انتخاب تصادفی 
$k$
عضو از 
$D$
جمله تخمینی از مجموع به دست آوریم
(با ضریب مقیاس 
$\frac{D}{k}$
). در خصوص ماتریس داده‌ی 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
\textit{
انتخاب تصادفی مختصات
}%
\LTRfootnote{Random coordinate sampling}
، $k$ ستون را از ماتریس داده به طور یکنواخت و تصادفی انتخاب می‌کند.

نمونه‌گیری از این جهت سودمند است که هم سایکل‌های کاری 
\lr{CPU}
را کاهش ‌می‌دهد و هم در حافظه صرفه‌جویی می‌کند. در کابردهای جدید، در اغلب موارد صرفه‌جویی در حافظه از اهمیت بیشتری برخوردار است. در نیم قرن گذشته گلوگاه‌ محاسباتی حافظه بوده است، نه پردازشگر. سرعت پردازشگرها با نرخ تقریبی ۷۵ درصد در سال رو به افزایش است. در حالی که سرعت حافظه تقریبا سالی ۷ درصد افزایش می‌یابد
\cite{litez139}
. این پدیده به عنوان «دیوار حافظه»%
\LTRfootnote{Memory wall}
شناخته می‌شود
‌\cite{litez139, litez168}
. بنابراین در کاربردهایی که شامل مجموعه داده‌های حجیم می‌شوند، بحرانی‌ترین کار بیان کردن داده‌ها است. برای مثال،
از طریق نمونه‌گیری با فرمی فشرده برای قرارگیری در ابعاد حافظه در دسترس.


\subsection{مزایای نمونه‌گیری تصادفی مختصات}
نمونه‌گیری تصادفی مختصات به دو دلیلی معمولا انتخاب پیش‌فرض است.

\begin{itemize}
\item
\textbf{سادگی}
این روش از لحاظ زمانی تنها از مرتبه 
$O(nk)$
برای نمونه‌گیری 
$k$ 
ستون از 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
طول می‌کشد.
\item
\textbf{انعطاف پذیری}
یک مجموعه نمونه را می‌توان برای تخمین بسیاری از شاخص‌های آماری استفاده کرد. شامل: ضرب داخلی، فاصله
$l_\alpha$
(برای هر مقداری از 
$\alpha$
)
\end{itemize}

\subsection{معایب نمونه‌گیری تصادفی مختصات}
با این حال نموننه‌گیری تصادفی مختصات دو ایراد اساسی دارد.
\begin{itemize}
\item
معمولا دقیق نیست زیرا مقادیری با مقدار زیاد محتمل است که گم شوند. مخصوصا زمانی که داده‌ها دم سنگینی داشته باشند. داده‌های بزرگ مقیاس دنیای واقعی (مخصوصا داده‌های مربوط به اینترنت) همیشه دم  سنگین هستند و از قاعده توانی پیروی می‌کنند.
\cite{litez142,litez66, litez53, litez111}
زمانی که فاصله 
$l_2$
یا ضرب داخلی را تخمین می‌زنیم. واریانس تخمین‌ها بر اساس ممان چهارم داده‌ها تعیین می‌شود. در حالی که در داده‌های دم سنگین، گاهی اوقات حتی ممان اول هم معنی‌دار نیست (محدود نیست)
\cite{litez142}
.
\item
این روش داده‌های پراکنده را به خوبی مدیریت نمی‌کند. بسیاری از داده‌های بزرگ مقیاس به شدت پراکنده هستند، به عنوان مثال، داده‌های متنی 
\cite{litez60}
و داده‌های بر اساس بازار
\cite{litez7, litez158}
. به جز برخی واژه‌های کاربردی مانند 
\lr{"A"}
 و 
\lr{"The"}
بیشتر لغات با نسبت بسیار کمی در مستندات ظاهر می‌شوند (
$<1\%$
)
اگر ما داده‌ها را با در نظر گرفتن تعدادی از ستون‌های ثابت نمونه‌گیری کنیم. خیلی محتمل است که بیشتر داده‌های (مقادیر غیر صفر) را از دست بدهیم.به خصوص موارد جذابی که دو مقدار با هم غیر صفر شده‌اند.
\end{itemize}
در این پایان‌نامه ما روش تصویر تصادفی را مورد بررسی قرار می‌دهیم و نشان خواهیم داد که این روش به خوبی قابلیت مدیریت داده‌های دم‌سنگین را دارد.

%========================================Section====================================
\section{تصویر تصادفی پایدار}
تصویر
\autoref{fig:matrixmulti}
، ایده تصویر تصادفی را نشان می‌دهد. ایده اصلی تصویر تصادفی ضرب ماتریس داده‌ی 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
در ماتریس تصادفی 
$\mathbf{R} \in \mathbb{R}^{D \times k} (k \ll D)$
است. که حاصل ماتریس تصویر شده‌ی 
$\mathbf{B} = \mathbf{A} \times \mathbf{R} \in \mathbb{R}^{n \times k}$
است. 
$\mathbf{B}$
بسیار کوچکتر از 
$\mathbf{A}$
است و بنابراین به راحتی قابل ذخیره‌سازی است. (برای مثال: برای حافظه‌های فیزیکی به اندازه‌ی کافی کوچک است)

\begin{figure}[h]
\centering
\begin{latin}
\begin{tikzpicture}
\fill[black!50!white] (0,1) rectangle (4,4);
\node at (2,2.5) [rectangle,preaction={fill=black!50},font={A}] {};
\node at (4.5,2.5) [rectangle,preaction={fill=black!0},font={$\times$}] {};
\fill[black!40!white] (5,0) rectangle (6,5);
\node at (5.5,2.5) [rectangle,preaction={fill=black!40},font={R}] {};
\node at (6.5,2.5) [rectangle,preaction={fill=black!0},font={=}] {};
\fill[black!60!white] (7,1) rectangle (8,4);
\node at (7.5,2.5) [rectangle,preaction={fill=black!60},font={B}] {};
\end{tikzpicture}
\end{latin}
\caption{
تصویر تصادفی پایدار 
$\mathbf{B} = \mathbf{A} \times \mathbf{R}$
،
$\mathbf{A}$
ماتریس اولیه داده‌ها است.
}
\label{fig:matrixmulti}
\end{figure}

ماتریس تصویرگر 
$\mathbf{R} \in \mathbb{R}^{D \times k}$
معمولا از داریه‌های مستقل هم توزیع (
\lr{i.i.d}
) یک توزیع متقارن $\alpha$-پایدار پر شده است.
\cite{litez171}
(بنابراین نام این روش «تصویر تصادفی پایدار» است.)
بر اساس مشخصات توزیع‌های $\alpha$-پایدار، داده‌های تصویر شده هم از توزیع $\alpha$-پایدار پیروی می‌کنند. که بر اساس آن‌ها شاخص‌های $l_\alpha$ و فاصله دودویی $l_\alpha$ در $\mathbf{A}$ تخمین زده می‌شوند و می‌توانیم داده‌های اصلی را دور بریزیم.

موفقیت تصویر پایدار تصادفی توسط لم 
\lr{Johnson-Lindenstrauss (JL)}
\cite{litez103}
برای کاهش بعد در $l_2$نشان داده شده است. لم
\lr{JL}
بیان می‌کند: رعایت
$k = O \left ( \frac{\log n}{\epsilon^2 } \right ) $
تضمین می‌کند هر فاصله $l_2$ میان $n$ نقطه در هر تعداد بعدی با دقت 
$1\pm\epsilon$
 با احتمال بالایی تخمین زده شود. ($k$ در اینجا بیانگر تعداد ابعاد کاهش یافته است)

با این حال لم 
\lr{JL}
برای نرم‌های فاصله با 
$\alpha$
کوچکتر از ۲ 
$l_\alpha ( \alpha < 2 )$
صادق نیست. در صورتی که لازم باشد از برآوردگرهایی استفاده کنیم که متریک باشند (در نامساوی مثلثی صدق کنند). به این نتیجه «عدم امکان»%
\LTRfootnote{Impossibility}
 گفته می‌شود.
\cite{litez39, litez109, litez33}
خوشبختانه شامل برآوردگرهایی که متریک نیستند نمی‌شود. در این پایان‌نامه ما در مورد برآوردگرهای کوناگونی که متریک نیستند صحبت خواهیم کرد. شامل: میانگین هندسی%
\LTRfootnote{Geometric mean}
، میانگین هارمونیک
‌\LTRfootnote{Harmonic mean}
، نسبت توانی%
\LTRfootnote{Fractional power}
و همچنین حداکثر بزرگنمایی.

%=====================================Section===================================
\section{کاربردها}
علاقه‌ی زیادی به تکنیک‌های نمونه برداری وجود دارد که در کاربردهای زیادی مورد استفاده قرار می‌گیرند. مانند: قانون وابستگی%
\LTRfootnote{Association rules}
\cite{litez30, litez31}
، خوشه‌بندی، بهینه‌سازی درخواست%
\LTRfootnote{Query optimization}
\cite{litez136, litez44}
، تشخیص تکراری%
\LTRfootnote{Duplicate detection}
\cite{litez34, litez28}
و بسیاری موارد دیگر. روش‌های نمونه بردار هر چه بیشتر و بیشتر برای مجموعه‌های بزرگتر اهمیت پیدا می‌کنند.

طرح برودر%
\LTRfootnote{Broder's sketch}
\cite{litez34}
در ابتدا برای تشخیص صفحات وب تکراری معرفی شد. 
\lr{URL}
های زیادی به 
\lr{HTML}
های مشابه (یا تقریبا مشابه) اشاره می‌کنند. جواب‌های تخمین زده شده به اندازه‌ی کافی خوب بودند. نیازی نبود تا همه تکراری‌ها پیدا شوند ولی کاربردی بود که تعداد زیدی از آن‌ها پیدا شوند، بدون اینکه بیش از ارزش آن از توان محاسباتی استفاده شود.

در کاربردهای بازیابی اطلاعات (
\lr{IR}
)%
\LTRfootnote{information retrieval}
معمولا گلوگاه حافظه‌ی فیزیکی است. زیرا مجموعه‌ی وب برای حافظه (
\lr{RAM}
) بسیار بزرگ است و از طرفی ما می‌خواهیم زمان گشتن به دنبال داده‌ّا بر روی دیسک را کمینه کنیم. زیرا زمان پاسخ به یک درخواست کلیدی است
\cite{litez29}
. به عنوان یک وسیله صرفه‌جویی در فضا، کاهش بعد یک ارائه فشرده از داده‌ها فراهم می‌کند که برای تولید جواب‌های تخمینی در حافظه فیزیکی مورد استفاده قرار می‌گیرند.

ما به بازدید صفحات وب اشاره‌ کردیم. اگر ما یک عبارت جستجوی دو کلمه‌ای داشته باشیم، می‌خواهیم بدانیم چه تعداد از صفحات هر دو کلمه را دارند. فرض می‌کنیم محاسبه‌ی از قبل و نگهداری بازدید صفحات غیر ممکن باشد. حداقل نه برای کلماتی که تکرار زیادی ندارند و سری‌های چند کلمه‌ای.

مرسوم است که در بازیابی اطلاعات با یک ماتریس بزرگ عبارت به ازای سند شروع کنیم که در آن مقادیر ورودی نشان‌دهنده‌ی وجود عبارت در متن است. بنا به کاربردهای خاص می‌توانیم بک اندیس معکوس %
\LTRfootnote{inverted index}
بسازیم و کلیتی از عبارات (برای تخمین ارتباط لغات) یا اسناد (برای تخمین شباهت اسناد) نگهداری کنیم.

\subsection{
کاوش قوانین وابستگی
}
تحلیل‌های مبتنی بر بازار و قوانین وابستگی 
\cite{litez8, litez9, litez10}
ابزارهای مناسبی برای کاوش پایگاه‌ داده‌های تجاری هستند. پایگاه داده‌های تجاری دارند روز به روز بزرگتر و گسسته‌تر می‌شوند.
\cite{litez7, litez158}
الگوریتم‌های مختلف نمونه‌برداری پیشنهاد شده است. نمونه برداری این امکان را فراهم می‌کند تا قواعد تخصیص را به صورت آنلاین برآورد کنیم. که می‌تواند مزایایی در کاربردهای خاص داشته باشد.

\subsection{
وابستگی جفتی همه (فاصله‌ها)
}
در کابردهای مختلفی شامل کلاسه‌بندی بر مبنای فاصله یا خوشه‌بندی و مدل‌سازی زبان با 
\lr{bi-gram}%
\LTRfootnote{litez48}
ما نیازمند محاسبه‌ی همه‌ی جفت تخصیص‌ها (یا فاصله‌ها) هستیم. ماتریس داده‌ی 
$\mathbf{A}$
شامل 
$n$
سطر و 
$D$
ستون داده شده است. محاسبه‌ی مستقیم
$\mathbf{AA}^T$
،
$O(n^2 D)$
هزینه بر است. یا به طور بهینه‌تر 
$O(n^2 \bar{f})$
که 
$\bar{f}$
تعداد میانگین مقادیر غیر صفر میان تمام سطرهای 
‌$\mathbf{A}$
است. محاسبه مستیم می‌تواند به شدت زمان‌بر باشد. همچنین، به طور خاص زمانی که ماتریس داده آنقدر بزرگ است که در حافظه فیزیکی جا نمی‌شود. محاسبه به طور خاص بسیار ناکارآمد خواهد بود.

\subsection{
تخمین فاصله‌ها به طور آنلاین
}
در حالی که ماتریس داده‌ی اولیه 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
ممکن است برای حافظه‌ی فیزیکی بسیار بزرگ باشد، نگهداری%
\LTRfootnote{Materializing}
همه فاصله‌های جفتی و وابستگی‌ها در 
$\mathbf{A}$
،
$O(n^2)$
فضا مصرف می‌کند. که می‌تواند برای حافظه‌ی فیزیکی بسیار بزرگتر باشد. در این میان وابستگی‌های چندتایی را کنار می‌گزاریم. در بسیاری از کاربردها نظیر یادگیری برخط، سیستم‌های توصیه آنلاین، تحلیل‌های بازار برخط و موتورهای جستجو، بهتر است که نمونه‌ها (
\lr{sketches}
) در حافظه نگهداری شوند و همه‌ی فاصله‌ها به طور آنلاین، زمانی که مورد نیاز باشد، محاسبه شوند.

\subsection{
بهینه‌سازی درخواست از پایگاه داده
}

در پایگاه داده‌ها یک وظیفه‌ی بسیار مهم تخمین 
\lr{join}
های چندراهی است، که تاثیر زیادی بر روی کارایی سیستم دارد
\cite{litez81}
. بر اساس تخمین دوراهی، سه‌راهی و حتی 
\lr{join}
هایی از مرتبه‌ی بالاتر، بهینه‌گرهای درخواست یک نقشه برای کمینه کردن تابع هزینه می‌سازند (برای مثال، نوشتن‌های میانی%
\LTRfootnote{Intermediate writes}
). بهینه بودن اهمیت بسیاری دارد زیرا مثلا نمی‌خواهیم زمان بیشتری برای بهینه‌سازی نقشه نسبت به زمان اجرای آن تلف کنیم.

ما از مثال 
\lr{Governator}
برای نمایش کاربرد تخمین دو و چند راهه برای بهینه کردن درخواست استفاده می‌کنیم.

\begin{table}[h]
\centering
\begin{latin}
\begin{tabular}{lll}
\hhline{===}
        		 & Query		& Hits(Google) 	\\ \hline
\multirow{4}{*}{One-way} & Austria  	& 88,200,000 	\\
			 & Governor  	& 37,300,000 	\\
			 & Schwarzenegger & 4,030,000 	\\
			 & Terminator	 & 3,480,000 	\\ \hline
\multirow{6}{*}{Two-way} & Governor \& Schwarzenegger  	& 1,220,000 	\\ 
			 & Governor \& Austria  	& 708,000 	\\
			 & Schwarzenegger \& Terminator & 504,000 	\\
			 & Terminator \& Austria  	& 171,000 	\\
			 & Governor \& Terminator  	& 132,000 	\\
			 & Schwarzenegger \& Austria  	& 120,000 	\\ \hline
\multirow{4}{*}{Tree-way} & Governor \& Schwarzenegger \& Terminator  	& 75,100 \\
			  & Governor \& Schwarzenegger \& Austria  	& 46,100 \\
			  & Schwarzenegger \& Terminator \& Austria 	& 16,000 \\
			  & Governor \& Terminator \& Austria 		& 11,500 \\ \hline
\multirow{1}{*}{Four-way} & Governor \& Schwarzenegger \& Terminator \& Austria & 6,930	\\
\hhline{===}
\end{tabular}
\end{latin}
\caption{
بازدید صفحات گزارش شده توسط 
\lr{Google}
برای چهار کلمه و وابستگی‌های دو، سه و چهارتایی آن‌ها
}
\label{tab:governator}
\end{table}

\autoref{tab:governator}
بازدید صفحات را برای چهار کلمه و ترکیبات دو، سه، چهارتایی آن‌ها نشان می‌دهد. فرض کنیم بهینه‌ساز قصد استخراج نقشه برای درخواست:
$"\mathrm{Governor, Schwarzenegger, Terminator, Austria}"$
را داشته باشد. راه حل استاندارد این است که با عبارات با کمترین فراوانی شروع کند:
$(("\mathrm{Schwarzenegger}" \cap "\mathrm{Terminator}") \cap "\mathrm{Governor}") \cap "\mathrm{Austria}"$
این نقشه 
$579,100$
نوشتن میانی بعد از اولین و دومین 
\lr{join}
ها دارد. یک بهینه‌سازی می‌تواند 
$(("\mathrm{Schwarzenegger}" \cap "\mathrm{Austria}") \cap "\mathrm{Terminator}") \cap "\mathrm{Governor}"$
باشد که 
$579,100$
را به 
$136,000$
کاهش می‌دهد.


\subsection{
جستجوی نزدیکترین همسایه از مرتبه‌ی زیر خطی
}
محاسبه‌ی نزدیکترین همسایه در بسیاری کاربردها از اهمیت زیادی برخوردار است. با این حال، به دلیل «نفرین ابعاد»%
\LTRfootnote{Curse of dimensionality}
راه حل فعلی برای پیدا کردن بهینه‌ی نزدیکترین همسایه‌ها (حتی به طور تقریبی) اصلا رضایت بخش نیست.
\cite{litez88, litez100}

به دلیل ملاحظات محاسباتی، دو شکل اصلی در جستجوی نزدیکترین همسایه‌ها وجود دارد. اول اینکه ماتریس اصلی داده‌ها 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
ممکن است برای حافظه فیزیکی بسیار بزرگ باشد ولی اسکن کردن دیسک‌های سخت برای پیدا کردن نزدیکترین همسایه‌ها می‌تواند خیلی کند باشد. دوما، پیدا کردن نزدیکترین همسایه‌های یک داده ممکن است
$O(nD)$
هزینه بر باشد که می‌تواند به شدت زمان بر شود.

با این حال، روس کاهش ابعادی در این پایان‌نامه می‌تواند در حافظه صرفه‌جویی کند و سرعت محاسبات را افزایش دهد. برای مثال: وقتی ماتریس داده‌ی اولیه 
$\mathbf{A}$
به ماتریس داده‌ی 
$\mathbf{B} \in \mathbb{R}^{n \times k}$
کاهش می‌یابد. با این حال، 
$O(nk)$
و معمولا این درخواست وجود دارد که هزینه‌ی محاسباتی از 
$O(n)$
به 
$O(n^\gamma)$
برای 
$\gamma < 1$
کاهش پیدا کند، حداقل برای کاربردهای خاص.

دو گروه اصلی الگوریتم‌های زیر خطی برای محاسبه عبارتند از 
\lr{KD-Trees}
(و انواع آن)
\cite{litez79, litez80}
و 
\lr{Locality-Sensitive Hashing (LSH)}
\cite{litez15, litez56, litez100}
این الگوریتم‌ها معمولا با یک فضای متریک کار می‌کنند (که در ان نامساوی مثلثی برقرار است). برای مثال، فضای 
$l_\alpha$
زمانی که 
$\alpha \geq 1$
باشد یک متریک است. زمانی که به دنبال نزدیکترین‌ همسایه‌ها در 
$l_\alpha$
(
$\alpha > 1$
) میگردیم، می‌توانیم (نسبتا به سادگی) فضای جستجو را به طور کاملا اساسی با استفاده از نامساوی مثلثی کاهش دهیم. به عبارت دیگر، نیازی نیست که همه 
$n$
نقطه داده‌ها را مورد بررسی قرار دهیم.

در داده‌هایی با ابعاد بسیار بزرگ، الگوریتم‌های زیر خطی موجود شامل 
\lr{KD-trees}
و 
\lr{LSH}
، عملکرد رضایت بخشی ندارند.  وقتی حافظه‌ی فیزیکی (به جای 
\lr{CPU}
) گلوگاه باشد%
\LTRfootnote{Memory wall}
، یکی از مشکلات اصلی این است که این الگوریتم‌ها برای کاهش هزینه‌ی محاسباتی به حافظه‌ی ابر خطی%
\LTRfootnote{Super-linear memory}
 نیاز دارند که می‌تواند مشکل ساز باشد.
\cite{litez100}
به طرح کلی برای 
\lr{LSH}
توجه کنید که ترکیبی از هش%
\LTRfootnote{Hash}
و تصویر تصادفی است. متاسفانه این طرح به دلیل هزینه‌ی زیاد پیش پردازش غیر کاربردی است.
\cite{litez100}

در این پایان‌نامه، موفقیت اصلی کاهش بعد داده 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
به
$\mathbf{B} \in \mathbb{R}^{n \times k}$
و تامین برآوردگرهای دقیق برای استخراج فاصله‌های اولیه در 
$\mathbf{A}$
بر اساس
$\mathbf{B}$
است. در حالی که سناریوهای مهمی وجود دارند که در آن‌ها نتایج ما رضایت بخش هستند، توسعه‌ی یک الگوریتم زیر-خطی برای تخمین نزدیکترین همسایه‌ها، بر اساس الگوریتم ما یک ایده جذاب برای تحقیقات آینده است. یک مانع اصلی در این راه این است که  بیشتر برآوردگرهای ما غیر متریک هستند و بنابراین طراحی یک الگوریتم هوشمند و تحلیل‌های تئوری ممکن است سخت باشد، با این حال غیر ممکن نیست.





