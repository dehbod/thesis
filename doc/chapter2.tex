\chapter{مرور ادبیات}
\section{مقدمه}
عمومیت پیدا کردن داده‌های حجیم مانند داده‌های حجیم تحت وب و جریان‌های داده بزرگ در کاربردهای جدید،
موجب به وجود آمدن فرصت‌های و چالش‌هایی برای مهندسین و دانشمندان شده است.
\cite{li2007stable}
برای مثال، زمانی که ماتریس داده 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
ابعادی در حد وب داشته باشد، عملیات ساده‌ای مانند محاسبه
$\mathbf{A} \mathbf{A}^T$
سخت می‌شود.
برای ارائه و نگهداری داده‌های حجیم در حافظه‌ای کوچک و برای استخراج اطلاعات آماری اصلی از مجوعه‌ای از بیانی محدود، روش‌های گوناگونی نمونه‌برداری توسعه‌ یافته است. به طور کلی روش 
\textit{تصویر تصادفی پایدار}
\LTRfootnote{Stable Random Projection}
 برای داده‌های با دم سنگین خیلی خوب کار می‌کند.

روش تصویر تصادفی پایدار، ماتریس داده‌های اولیه 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
را در ماتریس تصادفی 
$\mathbf{R} \in \mathbb{R}^{D \times k}$
$(k \ll D)$
 ضرب می‌کند و نتیجه ماتریس
$\mathbf{B} = \mathbf{AR} \in \mathbb{R}^{n \times k}$
است. 
معمولا درایه‌های ماتریس تصادفی
$\mathbf{R}$
به صورت 
\lr{i.i.d}
\LTRfootnote{Independent and identically distributed random variables}
از یک توزیع 
$\alpha$
-پایدار متقارن انتخاب می‌شوند (
$ 0 < \alpha \leq 2$
).
ما می‌توانیم مشخصه‌های 
$l_\alpha$
را در 
$\mathbf{A}$
بر اساس 
$\mathbf{B}$
تخمین بزنیم. در مورد حالت 
$l_2$
مزیت توزیع تصادفی پایدار توسط لم 
\lr{JL}
\LTRfootnote{Johnson-Lindenstrauss}
برجسته شده است. لم 
\lr{JL}
بیان می‌دارد که کافی است 
$k=O(\frac{ \log  n}{\epsilon^ 2})$
باشد تا هم فاصله دو به دویی با نرم 
$l_\alpha$
در 
$\mathbf{A}$
را بتوان با ضریب 
$1 \pm \epsilon$
از روی ماتریس 
$\mathbf{B}$
تخمین زد. در تز 
\lr{Ping Li}
\cite{li2007stable}
لمی مشابه لم 
\lr{JL}
برای 
$0 < \alpha < 2$
اثبات شده است. روش تصویر تصادفی پایدار به یک مسئله تخمین آماری کاهش می‌یابد برای تخمین پارامتر مقیاس برای یک توضیع پایدار $\alpha$ متقارن. این مسئله از این جهت مورد توجه قرار می‌گیرد زیرا ما به دنبال برآوردی می‌گردیم که هم از نظر آماری درست باشند و هم از نظر محاسباتی مقرون به صرفه. برآوردگرهای مختلفی را مطالعه و مقایسه کردیم. شامل میانگین حسابی، میانگین هندسی، میانگین هارمونیک، تقسیم توانی
\LTRfootnote{fractional power}
و برآوردگر حداکثر بزرگنمایی.

در این پایان‌نامه ما به بررسی موارد خاصی از تصویر تصادفی پایدار می‌پردازیم. برای نرم 
$l_2$
ارتقایی را با استفاده از اطلاعات حاشیه‌ای پیشنهاد می‌کنیم. همچنین برای حالت $l_2$ می‌توان ماتریس تصویرگر را از یک توزیع زیرگوسی
\LTRfootnote{sub-Gaussian}
بسیار کوچکتر به جای توزیع نرمال انتخاب کرد. با در نظر گرفتن محدودیت‌های معقولی می‌توان، از یک توزیع خاص زیر گوسی استفاده کرد. این توزیع شامل 
$[-1,0,1]$
با احتمالات 
$\{ \frac{1}{s}, 1-\frac{1}{2s}, \frac{1}{s} \}$
با مقادیر بسیار بزرگی برای 
‌‌‌$s$
(به عبارتی، تصویر تصادف خیلی گسسته 
\LTRfootnote{Very sparse random projections}
) می‌تواند به خوبی تصویر تصادفی نرمال عمل کند. برای حالت نرم 
$l_1$
به عبارتی دیگر تصویر تصادفی کوچی 
\LTRfootnote{Cauchy random projections}
انجام تخمین کاری نسبتا جذاب است. برای مثال، محاسبه برآوردگر بیشینه درستنمایی 
\lr{MLE}
در این حالت از لحاظ محاسباتی ممکن است. و یک توزیع معکوس گاوسی 
\LTRfootnote{inverse Gaussian}
برای مدل‌سازی دقیق توزیع 
\lr{MLE}
بیان شده است.

روش تصویر تصادفی ار پراکندگی داده‌ها استفاده‌ای نمی‌کند. در حالی که داده‌های بزرگ مقیاس معمولا بسیار پراکنده هستند. از روش تصویر تصادفی می‌توان برای حل مسائل بزرگ مقیاس در علوم و مهندسی در موتورهای جستجو و سیستم‌های اخذ داده، پایگاه‌های داده، سیستم‌های جریان داده جدید، جبر خطی عددی و بسیاری از کارهای یادگیری ماشین و داده کاوی که شامل محاسبه حجیم فاصله‌ها است، استفاده کرد.




\section{داده‌های حجیم}
عبارات زیر از سایت 
\lr{\textit{Information Week}}
نقل قول شده‌اند
\LTRfootnote{http://www.informationweek.com/news/showArticle.jhtml?articleID=175801775}
:

\begin{itemize}
\item
مقدار داده‌ای که توسط کسب و کارها ذخیره می‌شود تقریبا هر ۱۲ تا ۱۸ ماه دو برابر می‌شود.
\item
پایگاه داده‌ها بیشتر هم ‌زمان شده‌اند. فروشگاه‌های زنجیره‌ای 
\lr{Wall-Marat}
داده‌های فروش را هر ساعت به روز می‌کند.
\item
اضافه شدن یک میلیون خط داده اجازه جستجوهای پیچیده‌تری را می‌دهد. شرکت 
\lr{EBay}
به کارمندان اجازه می‌دهد برای بدست آوردن درکی عمیق‌تر در خصوص رفتار مشتریان در میان داده‌های حراج در بازه‌های زمانی کوتاه جستجو کنند.
\item
بزرگترین پایگاه داده‌ها توسط، مرکز شتابدهنده خطی استاندارد، مرکز تحقیقات ناسا، آژانس امنیت ملی و ... در ابعادی در محدوده‌ی پتابایت (هزار ترابایت 
$10^15$
بایت)، اداره می‌شوند.
\end{itemize}

پدیده نو ظهور مجموعه‌ داده‌ّای حجیم، چالش‌های محاسباتی در بسیاری کاربردهای علمی و تجاری به وجود آورده است. شامل اخترفیزیک، بیوتکنولوزی، جمعیت شناسی
\LTRfootnote{demographics}
، مالی، سیستم‌های اطلاعات جغرافیایی، دولت، دارو، ارتباطات از راه دور، محیط زیست و اینترنت.

\subsection{داده‌های حجیم وب}

وب چقدر بزرگ است؟  
\autoref{tab:searchengin}
نشان‌دهنده تعداد بازدید صفحات در موتورهای جستجوی امروزی است. به طور تخمینی حدود 
$D = 10^10$
صفحه‌ی وب را می‌توان بر اساس بازدید دو واژه‌ی بسیار پر کاربرد «
\lr{A}
» و «
\lr{THE}
» تخمین زد. 
\autoref{tab:searchengin}
 همچنین نشان می‌دهد که حتی کلماتی که به ندرت کاربرد دارند هم تعداد زیادی بازدید دارند.

\begin{table}[h]
\centering
\begin{latin}
\begin{tabular}{lll}
\hline
Query        & Google         & Bing        \\ \hhline{===}
A            & 25,270,000,000 & 175,000,000 \\
The          & 25,270,000,000 & 101,000,000 \\
Kalevala     & 7,440,000      & 939,000     \\
Griseofulvin & 1,163,000      & 332,000     \\
Saccade      & 1,030,000      & 388,000     \\
\hline
\end{tabular}
\end{latin}
\caption{
تعداد بازدید صفحات برای کلمات با بازخورد بالا و کلمات با بازخورد نادر
}
\label{tab:searchengin}
\end{table}


کلماتی با بازخورد معمولی چه میزان بازدید دارند؟ برای جواب این سوال ما به طور تصادفی ۱۵ صفحه از لغتنامه‌‌ی آموزشی انتخاب می‌کنیم.
\cite{litez94}
(لغتنامه‌ای با ۵۷،۱۰۰ کلمه) و اولین کلمه در هر صفحه را مد نظر قرار می‌دهیم. میانه‌ی آماری بر اساس جستجو‌گر گوگل ۱۰ میلیون صفحه برای کلمه است.

زبان انگلیسی چند کلمه دارد؟ در اینجا عبارتی را از 
\lr{AskOxford.com}
نقل قول می‌کنیم:

« این بیان میدارد که حداقل یک چهارم میلیون واژه‌ی انگلیسی مستقل وجود دارد. به جز افعال صرفی و کلمات فنی و ناحیه‌ای که توسط 
\lr{OED}
\LTRfootnote{Oxford English Dictionary}
تحت پوشش قرار نمی‌گیرند یا کلماتی که هنوز به لغتنامه‌های منتشر شده اضافه نشده‌اند. در صورتی که این موارد هم در نظر گرفته شوند تعداد لغات در حدود سه چهارم میلیون لغت خواهد بود »

بنابراین اگر یک ماتریس «عبارت به سند» 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
در نظر بگیریم. در ابعاد وب این ماتریس در ابعاد 
$n \approx 10^6$
و 
$D \approx 10^10$
بزرگ خواهد شد.
در اینجا عدد 
$(i,j)$
در 
$\mathbf{A}$
تعداد ظهور واژه 
$i$
در سند 
$j$
را نشان می‌دهد.

کارکردن با ماترسی در این ابعاد بزرگ چالش برانگیز است. برای مثال، شاخص 
\lr{LSI}
\LTRfootnote{latent semantic indexing}
\cite{litez58}
و یک مدل موضوعی فراگیر، از 
\lr{SVD}
\LTRfootnote{singular value decomposition}
بر روی ماتریس عبارت به سند استفاده می‌کند. که انجام این عملیات در ابعاد وب قطعا غیرممکن است.

یک مشکل اصلی در قبال مجموعه داده‌های سنگین، حافظه کامپیوتر است. به این دلیل که ابعاد و سرعت حافظه فیزیکی بسیار رشد کمتری در مقایسه با پردازنده‌ها (
\lr{CPU}
) دارد. این پدیده به عنوان دیوار حافظه شناخته می‌شود
\cite{litez139, litez168}
. برای مثال،‌ هر چند ممکن است تمامی رخداد‌های همزمان دوتایی از پیش محاسبه شوند، ولی نگهداری این حجم از داده در حافظه غیر ممکن است. علاوه بر این، گاهی اوقات تخصیص‌‌هایی با بیش از دو عامل هم اهمیت پیدا می‌کنند زیرا درخواست‌ها ممکن است شامل بیش از دو واژه هم باشند. یک راه حل ممکن این است که یک «نمونه» از 
$\mathbf{A}$
 نگهداری شود و همزمانی‌ها بر اساس این نمونه در حین کار تخمین زده شوند. ما حدس می‌زنیم که این روش توسط موتورهای جستجوی امروزی مورد استفاده قرار می‌گیرد، هر چند که روش واقعی قطعا جزو اسرار تجاری آن‌ها است.

هر چند که انتظار می‌رود تخمین‌ها سازگار باشند و فرکانس‌های جفت شده باید با افزایش عبارت به درخواست، کاهش پیدا کنند. 
\autoref{tab:querylengthcon}
نشان می‌دهد که تخمین‌های بیان شده با موتورهای جستجوی فعلی، همیشه سازگار نیستند.
\begin{table}[h]
\centering
\begin{latin}
\begin{tabular}{lll}
\hhline{===}
Query        				& Hits(Bing)    & Hits(Google) 	\\ \hline
America            			& 150,731,182 	& 393,000,000 	\\
America \& China          		& 15,240,116  	& 66,000,000 	\\
America \& China \& Britain     	& 235,111     	& 6,090,000     \\
America \& CHina \& Britain \& Japan 	& 154,444     	& 23,300,000    \\
\hhline{===}
\end{tabular}
\end{latin}
\caption{
با افزایش تعداد عبارات در درخواست، باید فرکانس‌های جفت شده کاهش پیدا کنند. ولی تخمین‌های بیان شده توسط موتورهای جستجو گاهی این موضوع تثبیت شده را نقض می‌کنند.
}
\label{tab:querylengthcon}
\end{table}

با اینکه، تعداد کل واژه‌های انگلیسی ( که به‌طور صحیح نوشته شده‌اند) هم اکنون شگفت‌اور است، در بسیاری کاربردهای متن کاوی، ما باید با ابعاد بسیار بزرگتری سر و کار داشته باشیم. در حالی که یک سند ممکن است بیانگر برداری از تک واژه‌ها باشد (به عبارت دیگر، مدل کیسه لغات
\LTRfootnote{bag-of-words}
). معمولا بهتر است سند به عنوانن یک بردار از لغات به صورت 
\lr{l}
پیوسته 
\LTRfootnote{l-shingles}
\cite{litez34}
بیان شود. برای مثال، با استفاده از مدل ۳ پیوسته، جمله‌ی
\lr{"It is a nice day"}
به مجموعه‌ی زیر تجزیه می‌شود. 
\lr{\{"it is a", "is a nice", "a nice day"\}}
این مدل به طور جشمگیری ابعداد داده‌ها را افزایش می‌دهد. به خاطر اینکه، اگر مجموعه‌ی 
$10^6$
تک لغت انگلیسی موجود داشته باشد. مدل ۳ پیوسته تعداد ابعاد را از 
$10^6$
به 
$10^{18}$
افزایش می‌دهد.

\subsection{
جریان‌های داده‌ی حجیم
}
در بسیاری کاربردهای جدید پردازش داده، جریان‌های داده‌ی حجیم نقش بنیادی دارند. جریان‌های داده‌ای که از روترهای اینترنت، سوئیچ‌های تلفن، رصد امسفر، شبکه‌های سنسور، شرایط ترافیکی بزرگراهی، داده‌های مالی و غیره 
\cite{litez5, litez141, litez49, litez19, litez96, litez69, litez91}
حاصل می‌شوند.

برخلاف پایگاه‌ داده‌های سنتی، معمول نیست که جریان‌های داده‌ی حجیم (که با سرعت زیادی منتقل می‌شوند) در جای نگهداری شوند. بنابراین پردازش معمولا به طور همزمان انجام می‌شوند. برای مثال، گاهی اوقات «رصد تصویری» داده‌ها با رصد تغییرات زمانی برخی آماره‌ها کفایت می‌کند. برای مثال آماره‌های نظیر: مجموع، تعداد آیتم‌های مجزا، برخی نرم‌های 
$l_\alpha$
. در برخی کاربردها (برای مثال، طبقه‌بندی صدا/محتوا و جدا سازی) نیاز است یک مدل یادگیری آماری برای کلاسه‌بندی
\LTRfootnote{Classification}
یا خوشه‌بندی
\LTRfootnote{Clustering}
جریان داده‌های حجیم توین شود. ولی معمولا فقط می‌توانیم یک‌بار داده‌ها را مورد بررسی قرار دهیم.

یک خاصیت مهم جریان‌های داده‌ای این است که دینامیک هستند. به عنوان یک مدل محبوب، جریان 
$u$
شامل ورودی‌های 
$(i, u_i)$
است که 
$i = 1 to D$
. برای مثال،
$D = 2^{64}$
زمانی که جریان بیان‌گر 
\lr{IP}
آدرس‌ها است.
\footnote{
هرچند ما بیشتر اوقات تعداد دقیق ابعاد (
\lr{D}
) یک جریان داده را نمی‌دانیم ولی در بیشتر کاربردها کافی است حد بالایی محافظه‌کارانه‌ای را در نظر بگیریم. برای مثال 
$D = 2^{64}$
زمانی که جریان بیانگر 
\lr{IP}
های ورودی است. همچنین این یکی از دلایلی است که داده‌ها بسیار پراکنده هستند. به این نکته توجه داشته باشید که ابعاد بسیار بزرک تاثیری در محاسبه‌ی فاصله‌ها و نمونه‌گیری طی الگوریتم‌های معرفی شده در این پایان‌نامه ندارد.
}
ورودی‌ها ممکن است به هر ترتیبی باشند و ممکن است مرتبا به روز شوند. ذات دینامیک جریان داده‌های حجیم فرآیند نمونه‌گیری را بسیار چالش‌برانگیزتر ا زمانی ‌می‌کند که با داده‌های ایستا سر و کار داریم.

\section{
چالش‌های نمونه‌گیری از داده‌های حجیم
}
در حالی که مسائل جذاب و چالش‌برانگیزی با ورود داده‌های حجیم شکل گرفته‌اند، این پایان‌نامه بر روی توسعه‌ی روش‌های نمونه‌گیری برای محاسبه فاصله در داده‌هایی با ابعاد بسیار بالا با استفاده از حافظه محدود تمرکز دارد.

در کاربردهای مدل‌سازی آماری و یادگیری ماشین، در اغلب موارد به جای داده‌های اصلی به فاصله، به خوصو فاصله‌ی جفتی نیاز داریم. برای مثال، محاسبه ماتریس گرام
\LTRfootnote{Gram matrix}
$\mathbf{AA^T}$
در آمار و یادگیری ماشین معمول است. 
$\mathbf{AA^T}$
بیانگر همه‌ی ضرب‌های داخلی دوتایی در ماتریس داده‌ی 
‌$\mathbf{A}$
است.

دو داده‌ی 
$u_1, u_2 \in \mathbb{R}^D$
داده شده‌اند. ضرب داخلی آن‌ها ( که با 
$a$
نمایش داده می‌شود) و 
$l_\alpha$
(که با 
$d_{(\alpha)}$
نمایش داده می‌شوند با عبارات زیر تعریف می‌شوند:
\footnote{
ما فاصله $l_\alpha$ را به صورت 
$d_{(\alpha)} = \sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^\alpha$
تعریف کرده‌ایم. به جای اینکه به شکل
$\mathopen( \sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^\alpha \mathclose)^{1/\alpha}$
تعریف کنیم. زیرا شکل اول در کاربردهای عملی عمومیت بیشتری دارد. برای مثال، لم 
\lr{JL}
، در ادبیات معمولا به شکل توان دو $l_2$ بیان می‌شود. 
$\sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^2$
به جای 
$\mathopen( \sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^2 \mathclose)^{1/2}$
. در این پایان‌نامه، ما برای سادگی 
$\sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^2$
را «فاصله $l_2$» بیان می‌کنیم به جای «مربع فاصله‌ی $l_2$».
}
\begin{align}
a=u_1^T u_2 = \sum_{i=1}^{D} u_{1,i} u_{2,i} \\
d_{(\alpha)} = \sum_{i=1}^{D} \mathopen| u_1 - u_2 \mathclose|^\alpha 
\label{eq:1hP}
\end{align}

به این نکته توجه داشته باشید که هم ضرب داخلی و هم فاصله به شکل جمع
$D$
جمله تعریف می‌شوند. بنابراین، زمانی که داده‌ها به اندازه‌ای حجیم هستند که نمی‌توان به طور کارا آن‌ها را مدیریت کرد، نمونه‌گیری خیلی عادی به نظر می‌رسد تا بتوان با انتخاب تصادفی 
$k$
عضو از 
$D$
جمله تخمینی از مجموع به دست آوریم
(با ضریب مقیاس 
$\frac{D}{k}$
). در خصوص ماتریس داده‌ی 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
\textit{
انتخاب تصادفی مختصات
}
\LTRfootnote{Random coordinate sampling}
، $k$ ستون را از ماتریس داده به طور یکنواخت و تصادفی انتخاب می‌کند.

نمونه‌گیری از این جهت سودمند است که هم سایکل‌های کاری 
\lr{CPU}
را کاهش ‌می‌دهد و هم در حافظه صرفه‌جویی می‌کند. در کابردهای جدید، در اغلب موارد صرفه‌جویی در حافظه از اهمیت بیشتری برخوردار است. در نیم قرن گذشته گلوگاه‌ محاسباتی حافظه بوده است، نه پردازشگر. سرعت پردازشگرها با نرخ تقریبی ۷۵ درصد در سال رو به افزایش است. در حالی که سرعت حافظه تقریبا سالی ۷ درصد افزایش می‌یابد
\cite{litez139}
. این پدیده به عنوان «دیوار حافظه»
\LTRfootnote{Memory wall}
شناخته می‌شود
‌\cite{litez139, litez168}
. بنابراین در کاربردهایی که شامل مجموعه داده‌های حجیم می‌شوند، بحرانی‌ترین کار بیان کردن داده‌ها است. برای مثال،
از طریق نمونه‌گیری با فرمی فشرده برای قرارگیری در ابعاد حافظه در دسترس.


\subsection{مزایای نمونه‌گیری تصادفی مختصات}
نمونه‌گیری تصادفی مختصات به دو دلیلی معمولا انتخاب پیش‌فرض است.

\begin{itemize}
\item
\textbf{سادگی}
این روش از لحاظ زمانی تنها از مرتبه 
$O(nk)$
برای نمونه‌گیری 
$k$ 
ستون از 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
طول می‌کشد.
\item
\textbf{انعطاف پذیری}
یک مجموعه نمونه را می‌توان برای تخمین بسیاری از شاخص‌های آماری استفاده کرد. شامل: ضرب داخلی، فاصله
$l_\alpha$
(برای هر مقداری از 
$\alpha$
)
\end{itemize}

\subsection{معایب نمونه‌گیری تصادفی مختصات}
با این حال نموننه‌گیری تصادفی مختصات دو ایراد اساسی دارد.
\begin{itemize}
\item
معمولا دقیق نیست زیرا مقادیری با مقدار زیاد محتمل است که گم شوند. مخصوصا زمانی که داده‌ها دم سنگینی داشته باشند. داده‌های بزرگ مقیاس دنیای واقعی (مخصوصا داده‌های مربوط به اینترنت) همیشه دم  سنگین هستند و از قاعده توانی پیروی می‌کنند.
\cite{litez142,litez66, litez53, litez111}
زمانی که فاصله 
$l_2$
یا ضرب داخلی را تخمین می‌زنیم. واریانس تخمین‌ها بر اساس ممان چهارم داده‌ها تعیین می‌شود. در حالی که در داده‌های دم سنگین، گاهی اوقات حتی ممان اول هم معنی‌دار نیست (محدود نیست)
\cite{litez142}
.
\item
این روش داده‌های پراکنده را به خوبی مدیریت نمی‌کند. بسیاری از داده‌های بزرگ مقیاس به شدت پراکنده هستند، به عنوان مثال، داده‌های متنی 
\cite{litez60}
و داده‌های بر اساس بازار
\cite{litez7, litez158}
. به جز برخی واژه‌های کاربردی مانند 
\lr{"A"}
 و 
\lr{"The"}
بیشتر لغات با نسبت بسیار کمی در مستندات ظاهر می‌شوند (
$<1\%$
)
اگر ما داده‌ها را با در نظر گرفتن تعدادی از ستون‌های ثابت نمونه‌گیری کنیم. خیلی محتمل است که بیشتر داده‌های (مقادیر غیر صفر) را از دست بدهیم.به خصوص موارد جذابی که دو مقدار با هم غیر صفر شده‌اند.
\end{itemize}
در این پایان‌نامه ما روش تصویر تصادفی را مورد بررسی قرار می‌دهیم و نشان خواهیم داد که این روش به خوبی قابلیت مدیریت داده‌های دم‌سنگین را دارد.

\section{تصویر تصادفی پایدار}
تصویر
\autoref{fig:matrixmulti}
، ایده تصویر تصادفی را نشان می‌دهد. ایده اصلی تصویر تصادفی ضرب ماتریس داده‌ی 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
در ماتریس تصادفی 
$\mathbf{R} \in \mathbb{R}^{D \times k} (k \ll D)$
است. که حاصل ماتریس تصویر شده‌ی 
$\mathbf{B} = \mathbf{A} \times \mathbf{R} \in \mathbb{R}^{n \times k}$
است. 
$\mathbf{B}$
بسیار کوچکتر از 
$\mathbf{A}$
است و بنابراین به راحتی قابل ذخیره‌سازی است. (برای مثال: برای حافظه‌های فیزیکی به اندازه‌ی کافی کوچک است)

\begin{figure}[h]
\centering
\begin{latin}
\begin{tikzpicture}
\fill[black!50!white] (0,1) rectangle (4,4);
\node at (2,2.5) [rectangle,preaction={fill=black!50},font={A}] {};
\node at (4.5,2.5) [rectangle,preaction={fill=black!0},font={$\times$}] {};
\fill[black!40!white] (5,0) rectangle (6,5);
\node at (5.5,2.5) [rectangle,preaction={fill=black!40},font={R}] {};
\node at (6.5,2.5) [rectangle,preaction={fill=black!0},font={=}] {};
\fill[black!60!white] (7,1) rectangle (8,4);
\node at (7.5,2.5) [rectangle,preaction={fill=black!60},font={B}] {};
\end{tikzpicture}
\end{latin}
\caption{
تصویر تصادفی پایدار 
$\mathbf{B} = \mathbf{A} \times \mathbf{R}$
،
$\mathbf{A}$
ماتریس اولیه داده‌ها است.
}
\label{fig:matrixmulti}
\end{figure}

ماتریس تصویرگر 
$\mathbf{R} \in \mathbb{R}^{D \times k}$
معمولا از داریه‌های مستقل هم توزیع (
\lr{i.i.d}
) یک توزیع متقارن $\alpha$-پایدار پر شده است.
\cite{litez171}
(بنابراین نام این روش «تصویر تصادفی پایدار» است.)
بر اساس مشخصات توزیع‌های $\alpha$-پایدار، داده‌های تصویر شده هم از توزیع $\alpha$-پایدار پیروی می‌کنند. که بر اساس آن‌ها شاخص‌های $l_\alpha$ و فاصله دودویی $l_\alpha$ در $\mathbf{A}$ تخمین زده می‌شوند و می‌توانیم داده‌های اصلی را دور بریزیم.

موفقیت تصویر پایدار تصادفی توسط لم 
\lr{Johnson-Lindenstrauss (JL)}
\cite{litez103}
برای کاهش بعد در $l_2$نشان داده شده است. لم
\lr{JL}
بیان می‌کند: رعایت
$k = O \left ( \frac{\log n}{\epsilon^2 } \right ) $
تضمین می‌کند هر فاصله $l_2$ میان $n$ نقطه در هر تعداد بعدی با دقت 
$1\pm\epsilon$
 با احتمال بالایی تخمین زده شود. ($k$ در اینجا بیانگر تعداد ابعاد کاهش یافته است)

با این حال لم 
\lr{JL}
برای نرم‌های فاصله با 
$\alpha$
کوچکتر از ۲ 
$l_\alpha ( \alpha < 2 )$
صادق نیست. در صورتی که لازم باشد از برآوردگرهایی استفاده کنیم که متریک باشند (در نامساوی مثلثی صدق کنند). به این نتیجه «عدم امکان»
\LTRfootnote{Impossibility}
 گفته می‌شود.
\cite{litez39, litez109, litez33}
خوشبختانه شامل برآوردگرهایی که متریک نیستند نمی‌شود. در این پایان‌نامه ما در مورد برآوردگرهای کوناگونی که متریک نیستند صحبت خواهیم کرد. شامل: میانگین هندسی
\LTRfootnote{Geometric mean}
، میانگین هارمونیک
‌\LTRfootnote{Harmonic mean}
، نسبت توانی
\LTRfootnote{Fractional power}
و همچنین حداکثر بزرگنمایی.

\section{کاربردها}
علاقه‌ی زیادی به تکنیک‌های نمونه برداری وجود دارد که در کاربردهای زیادی مورد استفاده قرار می‌گیرند. مانند: قانون وابستگی
\LTRfootnote{Association rules}
\cite{litez30, litez31}
، خوشه‌بندی، بهینه‌سازی درخواست
\LTRfootnote{Query optimization}
\cite{litez136, litez44}
، تشخیص تکراری
\LTRfootnote{Duplicate detection}
\cite{litez34, litez28}
و بسیاری موارد دیگر. روش‌های نمونه بردار هر چه بیشتر و بیشتر برای مجموعه‌های بزرگتر اهمیت پیدا می‌کنند.

طرح برودر
\LTRfootnote{Broder's sketch}
\cite{litez34}
در ابتدا برای تشخیص صفحات وب تکراری معرفی شد. 
\lr{URL}
های زیادی به 
\lr{HTML}
های مشابه (یا تقریبا مشابه) اشاره می‌کنند. جواب‌های تخمین زده شده به اندازه‌ی کافی خوب بودند. نیازی نبود تا همه تکراری‌ها پیدا شوند ولی کاربردی بود که تعداد زیدی از آن‌ها پیدا شوند، بدون اینکه بیش از ارزش آن از توان محاسباتی استفاده شود.

در کاربردهای بازیابی اطلاعات (
\lr{IR}
)
\LTRfootnote{information retrieval}
معمولا گلوگاه حافظه‌ی فیزیکی است. زیرا مجموعه‌ی وب برای حافظه (
\lr{RAM}
) بسیار بزرگ است و از طرفی ما می‌خواهیم زمان گشتن به دنبال داده‌ّا بر روی دیسک را کمینه کنیم. زیرا زمان پاسخ به یک درخواست کلیدی است
\cite{litez29}
. به عنوان یک وسیله صرفه‌جویی در فضا، کاهش بعد یک ارائه فشرده از داده‌ها فراهم می‌کند که برای تولید جواب‌های تخمینی در حافظه فیزیکی مورد استفاده قرار می‌گیرند.

ما به بازدید صفحات وب اشاره‌ کردیم. اگر ما یک عبارت جستجوی دو کلمه‌ای داشته باشیم، می‌خواهیم بدانیم چه تعداد از صفحات هر دو کلمه را دارند. فرض می‌کنیم محاسبه‌ی از قبل و نگهداری بازدید صفحات غیر ممکن باشد. حداقل نه برای کلماتی که تکرار زیادی ندارند و سری‌های چند کلمه‌ای.

مرسوم است که در بازیابی اطلاعات با یک ماتریس بزرگ عبارت به ازای سند شروع کنیم که در آن مقادیر ورودی نشان‌دهنده‌ی وجود عبارت در متن است. بنا به کاربردهای خاص می‌توانیم بک اندیس معکوس 
\LTRfootnote{inverted index}
بسازیم و کلیتی از عبارات (برای تخمین ارتباط لغات) یا اسناد (برای تخمین شباهت اسناد) نگهداری کنیم.

\subsection{
کاوش قوانین وابستگی
}
تحلیل‌های مبتنی بر بازار و قوانین وابستگی 
\cite{litez8, litez9, litez10}
ابزارهای مناسبی برای کاوش پایگاه‌ داده‌های تجاری هستند. پایگاه داده‌های تجاری دارند روز به روز بزرگتر و گسسته‌تر می‌شوند.
\cite{litez7, litez158}
الگوریتم‌های مختلف نمونه‌برداری پیشنهاد شده است. نمونه برداری این امکان را فراهم می‌کند تا قواعد تخصیص را به صورت آنلاین برآورد کنیم. که می‌تواند مزایایی در کاربردهای خاص داشته باشد.

\subsection{
وابستگی جفتی همه (فاصله‌ها)
}
در کابردهای مختلفی شامل کلاسه‌بندی بر مبنای فاصله یا خوشه‌بندی و مدل‌سازی زبان با 
\lr{bi-gram}
\LTRfootnote{litez48}
ما نیازمند محاسبه‌ی همه‌ی جفت تخصیص‌ها (یا فاصله‌ها) هستیم. ماتریس داده‌ی 
$\mathbf{A}$
شامل 
$n$
سطر و 
$D$
ستون داده شده است. محاسبه‌ی مستقیم
$\mathbf{AA}^T$
،
$O(n^2 D)$
هزینه بر است. یا به طور بهینه‌تر 
$O(n^2 \bar{f})$
که 
$\bar{f}$
تعداد میانگین مقادیر غیر صفر میان تمام سطرهای 
‌$\mathbf{A}$
است. محاسبه مستیم می‌تواند به شدت زمان‌بر باشد. همچنین، به طور خاص زمانی که ماتریس داده آنقدر بزرگ است که در حافظه فیزیکی جا نمی‌شود. محاسبه به طور خاص بسیار ناکارآمد خواهد بود.

\subsection{
تخمین فاصله‌ها به طور آنلاین
}
در حالی که ماتریس داده‌ی اولیه 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
ممکن است برای حافظه‌ی فیزیکی بسیار بزرگ باشد، نگهداری
\LTRfootnote{Materializing}
همه فاصله‌های جفتی و وابستگی‌ها در 
$\mathbf{A}$
،
$O(n^2)$
فضا مصرف می‌کند. که می‌تواند برای حافظه‌ی فیزیکی بسیار بزرگتر باشد. در این میان وابستگی‌های چندتایی را کنار می‌گزاریم. در بسیاری از کاربردها نظیر یادگیری برخط، سیستم‌های توصیه آنلاین، تحلیل‌های بازار برخط و موتورهای جستجو، بهتر است که نمونه‌ها (
\lr{sketches}
) در حافظه نگهداری شوند و همه‌ی فاصله‌ها به طور آنلاین، زمانی که مورد نیاز باشد، محاسبه شوند.

\subsection{
بهینه‌سازی درخواست از پایگاه داده
}

در پایگاه داده‌ها یک وظیفه‌ی بسیار مهم تخمین 
\lr{join}
های چندراهی است، که تاثیر زیادی بر روی کارایی سیستم دارد
\cite{litez81}
. بر اساس تخمین دوراهی، سه‌راهی و حتی 
\lr{join}
هایی از مرتبه‌ی بالاتر، بهینه‌گرهای درخواست یک نقشه برای کمینه کردن تابع هزینه می‌سازند (برای مثال، نوشتن‌های میانی
\LTRfootnote{Intermediate writes}
). بهینه بودن اهمیت بسیاری دارد زیرا مثلا نمی‌خواهیم زمان بیشتری برای بهینه‌سازی نقشه نسبت به زمان اجرای آن تلف کنیم.

ما از مثال 
\lr{Governator}
برای نمایش کاربرد تخمین دو و چند راهه برای بهینه کردن درخواست استفاده می‌کنیم.

\begin{table}[h]
\centering
\begin{latin}
\begin{tabular}{lll}
\hhline{===}
        		 & Query		& Hits(Google) 	\\ \hline
\multirow{4}{*}{One-way} & Austria  	& 88,200,000 	\\
			 & Governor  	& 37,300,000 	\\
			 & Schwarzenegger & 4,030,000 	\\
			 & Terminator	 & 3,480,000 	\\ \hline
\multirow{6}{*}{Two-way} & Governor \& Schwarzenegger  	& 1,220,000 	\\ 
			 & Governor \& Austria  	& 708,000 	\\
			 & Schwarzenegger \& Terminator & 504,000 	\\
			 & Terminator \& Austria  	& 171,000 	\\
			 & Governor \& Terminator  	& 132,000 	\\
			 & Schwarzenegger \& Austria  	& 120,000 	\\ \hline
\multirow{4}{*}{Tree-way} & Governor \& Schwarzenegger \& Terminator  	& 75,100 \\
			  & Governor \& Schwarzenegger \& Austria  	& 46,100 \\
			  & Schwarzenegger \& Terminator \& Austria 	& 16,000 \\
			  & Governor \& Terminator \& Austria 		& 11,500 \\ \hline
\multirow{1}{*}{Four-way} & Governor \& Schwarzenegger \& Terminator \& Austria & 6,930	\\
\hhline{===}
\end{tabular}
\end{latin}
\caption{
بازدید صفحات گزارش شده توسط 
\lr{Google}
برای چهار کلمه و وابستگی‌های دو، سه و چهارتایی آن‌ها
}
\label{tab:governator}
\end{table}

\autoref{tab:governator}
بازدید صفحات را برای چهار کلمه و ترکیبات دو، سه، چهارتایی آن‌ها نشان می‌دهد. فرض کنیم بهینه‌ساز قصد استخراج نقشه برای درخواست:
$"\mathrm{Governor, Schwarzenegger, Terminator, Austria}"$
را داشته باشد. راه حل استاندارد این است که با عبارات با کمترین فراوانی شروع کند:
$(("\mathrm{Schwarzenegger}" \cap "\mathrm{Terminator}") \cap "\mathrm{Governor}") \cap "\mathrm{Austria}"$
این نقشه 
$579,100$
نوشتن میانی بعد از اولین و دومین 
\lr{join}
ها دارد. یک بهینه‌سازی می‌تواند 
$(("\mathrm{Schwarzenegger}" \cap "\mathrm{Austria}") \cap "\mathrm{Terminator}") \cap "\mathrm{Governor}"$
باشد که 
$579,100$
را به 
$136,000$
کاهش می‌دهد.


\subsection{
جستجوی نزدیکترین همسایه از مرتبه‌ی زیر خطی
}
محاسبه‌ی نزدیکترین همسایه در بسیاری کاربردها از اهمیت زیادی برخوردار است. با این حال، به دلیل «نفرین ابعاد»
\LTRfootnote{Curse of dimensionality}
راه حل فعلی برای پیدا کردن بهینه‌ی نزدیکترین همسایه‌ها (حتی به طور تقریبی) اصلا رضایت بخش نیست.
\cite{litez88, litez100}

به دلیل ملاحظات محاسباتی، دو شکل اصلی در جستجوی نزدیکترین همسایه‌ها وجود دارد. اول اینکه ماتریس اصلی داده‌ها 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
ممکن است برای حافظه فیزیکی بسیار بزرگ باشد ولی اسکن کردن دیسک‌های سخت برای پیدا کردن نزدیکترین همسایه‌ها می‌تواند خیلی کند باشد. دوما، پیدا کردن نزدیکترین همسایه‌های یک داده ممکن است
$O(nD)$
هزینه بر باشد که می‌تواند به شدت زمان بر شود.

با این حال، روس کاهش ابعادی در این پایان‌نامه می‌تواند در حافظه صرفه‌جویی کند و سرعت محاسبات را افزایش دهد. برای مثال: وقتی ماتریس داده‌ی اولیه 
$\mathbf{A}$
به ماتریس داده‌ی 
$\mathbf{B} \in \mathbb{R}^{n \times k}$
کاهش می‌یابد. با این حال، 
$O(nk)$
و معمولا این درخواست وجود دارد که هزینه‌ی محاسباتی از 
$O(n)$
به 
$O(n^\gamma)$
برای 





