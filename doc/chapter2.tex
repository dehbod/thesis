\chapter{مرور ادبیات}
\section{مقدمه}
عمومیت پیدا کردن داده‌های حجیم مانند داده‌های حجیم تحت وب و جریان‌های داده بزرگ در کاربردهای جدید،
موجب به وجود آمدن فرصت‌های و چالش‌هایی برای مهندسین و دانشمندان شده است.
\cite{li2007stable}
برای مثال، زمانی که ماتریس داده 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
ابعادی در حد وب داشته باشد، عملیات ساده‌ای مانند محاسبه
$\mathbf{A} \mathbf{A}^T$
سخت می‌شود.
برای ارائه و نگهداری داده‌های حجیم در حافظه‌ای کوچک و برای استخراج اطلاعات آماری اصلی از مجوعه‌ای از بیانی محدود، روش‌های گوناگونی نمونه‌برداری توسعه‌ یافته است. به طور کلی روش 
\textit{تصویر تصادفی پایدار}
\LTRfootnote{Stable Random Projection}
 برای داده‌های با دم سنگین خیلی خوب کار می‌کند.

روش تصویر تصادفی پایدار، ماتریس داده‌های اولیه 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
را در ماتریس تصادفی 
$\mathbf{R} \in \mathbb{R}^{D \times k}$
$(k \ll D)$
 ضرب می‌کند و نتیجه ماتریس
$\mathbf{B} = \mathbf{AR} \in \mathbb{R}^{n \times k}$
است. 
معمولا درایه‌های ماتریس تصادفی
$\mathbf{R}$
به صورت 
\lr{i.i.d}
\LTRfootnote{Independent and identically distributed random variables}
از یک توزیع 
$\alpha$
-پایدار متقارن انتخاب می‌شوند (
$ 0 < \alpha \leq 2$
).
ما می‌توانیم مشخصه‌های 
$l_\alpha$
را در 
$\mathbf{A}$
بر اساس 
$\mathbf{B}$
تخمین بزنیم. در مورد حالت 
$l_2$
مزیت توزیع تصادفی پایدار توسط لم 
\lr{JL}
\LTRfootnote{Johnson-Lindenstrauss}
برجسته شده است. لم 
\lr{JL}
بیان می‌دارد که کافی است 
$k=O(\frac{ \log  n}{\epsilon^ 2})$
باشد تا هم فاصله دو به دویی با نرم 
$l_\alpha$
در 
$\mathbf{A}$
را بتوان با ضریب 
$1 \pm \epsilon$
از روی ماتریس 
$\mathbf{B}$
تخمین زد. در تز 
\lr{Ping Li}
\cite{li2007stable}
لمی مشابه لم 
\lr{JL}
برای 
$0 < \alpha < 2$
اثبات شده است. روش تصویر تصادفی پایدار به یک مسئله تخمین آماری کاهش می‌یابد برای تخمین پارامتر مقیاس برای یک توضیع پایدار $\alpha$ متقارن. این مسئله از این جهت مورد توجه قرار می‌گیرد زیرا ما به دنبال برآوردی می‌گردیم که هم از نظر آماری درست باشند و هم از نظر محاسباتی مقرون به صرفه. برآوردگرهای مختلفی را مطالعه و مقایسه کردیم. شامل میانگین حسابی، میانگین هندسی، میانگین هارمونیک، تقسیم توانی
\LTRfootnote{fractional power}
و برآوردگر حداکثر بزرگنمایی.

در این پایان‌نامه ما به بررسی موارد خاصی از تصویر تصادفی پایدار می‌پردازیم. برای نرم 
$l_2$
ارتقایی را با استفاده از اطلاعات حاشیه‌ای پیشنهاد می‌کنیم. همچنین برای حالت $l_2$ می‌توان ماتریس تصویرگر را از یک توزیع زیرگوسی
\LTRfootnote{sub-Gaussian}
بسیار کوچکتر به جای توزیع نرمال انتخاب کرد. با در نظر گرفتن محدودیت‌های معقولی می‌توان، از یک توزیع خاص زیر گوسی استفاده کرد. این توزیع شامل 
$[-1,0,1]$
با احتمالات 
$\{ \frac{1}{s}, 1-\frac{1}{2s}, \frac{1}{s} \}$
با مقادیر بسیار بزرگی برای 
‌‌‌$s$
(به عبارتی، تصویر تصادف خیلی گسسته 
\LTRfootnote{Very sparse random projections}
) می‌تواند به خوبی تصویر تصادفی نرمال عمل کند. برای حالت نرم 
$l_1$
به عبارتی دیگر تصویر تصادفی کوچی 
\LTRfootnote{Cauchy random projections}
انجام تخمین کاری نسبتا جذاب است. برای مثال، محاسبه برآوردگر بیشینه درستنمایی 
\lr{MLE}
در این حالت از لحاظ محاسباتی ممکن است. و یک توزیع معکوس گاوسی 
\LTRfootnote{inverse Gaussian}
برای مدل‌سازی دقیق توزیع 
\lr{MLE}
بیان شده است.

روش تصویر تصادفی ار پراکندگی داده‌ها استفاده‌ای نمی‌کند. در حالی که داده‌های بزرگ مقیاس معمولا بسیار پراکنده هستند. از روش تصویر تصادفی می‌توان برای حل مسائل بزرگ مقیاس در علوم و مهندسی در موتورهای جستجو و سیستم‌های اخذ داده، پایگاه‌های داده، سیستم‌های جریان داده جدید، جبر خطی عددی و بسیاری از کارهای یادگیری ماشین و داده کاوی که شامل محاسبه حجیم فاصله‌ها است، استفاده کرد.




\section{داده‌های حجیم}
عبارات زیر از سایت 
\lr{\textit{Information Week}}
نقل قول شده‌اند
\LTRfootnote{http://www.informationweek.com/news/showArticle.jhtml?articleID=175801775}
:

\begin{itemize}
\item
مقدار داده‌ای که توسط کسب و کارها ذخیره می‌شود تقریبا هر ۱۲ تا ۱۸ ماه دو برابر می‌شود.
\item
پایگاه داده‌ها بیشتر هم ‌زمان شده‌اند. فروشگاه‌های زنجیره‌ای 
\lr{Wall-Marat}
داده‌های فروش را هر ساعت به روز می‌کند.
\item
اضافه شدن یک میلیون خط داده اجازه جستجوهای پیچیده‌تری را می‌دهد. شرکت 
\lr{EBay}
به کارمندان اجازه می‌دهد برای بدست آوردن درکی عمیق‌تر در خصوص رفتار مشتریان در میان داده‌های حراج در بازه‌های زمانی کوتاه جستجو کنند.
\item
بزرگترین پایگاه داده‌ها توسط، مرکز شتابدهنده خطی استاندارد، مرکز تحقیقات ناسا، آژانس امنیت ملی و ... در ابعادی در محدوده‌ی پتابایت (هزار ترابایت 
$10^15$
بایت)، اداره می‌شوند.
\end{itemize}

پدیده نو ظهور مجموعه‌ داده‌ّای حجیم، چالش‌های محاسباتی در بسیاری کاربردهای علمی و تجاری به وجود آورده است. شامل اخترفیزیک، بیوتکنولوزی، جمعیت شناسی
\LTRfootnote{demographics}
، مالی، سیستم‌های اطلاعات جغرافیایی، دولت، دارو، ارتباطات از راه دور، محیط زیست و اینترنت.

\subsection{داده‌های حجیم وب}

وب چقدر بزرگ است؟  
\autoref{tab:searchengin}
نشان‌دهنده تعداد بازدید صفحات در موتورهای جستجوی امروزی است. به طور تخمینی حدود 
$D = 10^10$
صفحه‌ی وب را می‌توان بر اساس بازدید دو واژه‌ی بسیار پر کاربرد «
\lr{A}
» و «
\lr{THE}
» تخمین زد. 
\autoref{tab:searchengin}
 همچنین نشان می‌دهد که حتی کلماتی که به ندرت کاربرد دارند هم تعداد زیادی بازدید دارند.

\begin{table}[h]
\centering
\begin{latin}
\begin{tabular}{lll}
\hline
Query        & Google         & Bing        \\ \hhline{===}
A            & 25,270,000,000 & 175,000,000 \\
The          & 25,270,000,000 & 101,000,000 \\
Kalevala     & 7,440,000      & 939,000     \\
Griseofulvin & 1,163,000      & 332,000     \\
Saccade      & 1,030,000      & 388,000     \\
\hline
\end{tabular}
\end{latin}
\caption{
تعداد بازدید صفحات برای کلمات با بازخورد بالا و کلمات با بازخورد نادر
}
\label{tab:searchengin}
\end{table}


کلماتی با بازخورد معمولی چه میزان بازدید دارند؟ برای جواب این سوال ما به طور تصادفی ۱۵ صفحه از لغتنامه‌‌ی آموزشی انتخاب می‌کنیم.
\cite{litez94}
(لغتنامه‌ای با ۵۷،۱۰۰ کلمه) و اولین کلمه در هر صفحه را مد نظر قرار می‌دهیم. میانه‌ی آماری بر اساس جستجو‌گر گوگل ۱۰ میلیون صفحه برای کلمه است.

زبان انگلیسی چند کلمه دارد؟ در اینجا عبارتی را از 
\lr{AskOxford.com}
نقل قول می‌کنیم:

« این بیان میدارد که حداقل یک چهارم میلیون واژه‌ی انگلیسی مستقل وجود دارد. به جز افعال صرفی و کلمات فنی و ناحیه‌ای که توسط 
\lr{OED}
\LTRfootnote{Oxford English Dictionary}
تحت پوشش قرار نمی‌گیرند یا کلماتی که هنوز به لغتنامه‌های منتشر شده اضافه نشده‌اند. در صورتی که این موارد هم در نظر گرفته شوند تعداد لغات در حدود سه چهارم میلیون لغت خواهد بود »

بنابراین اگر یک ماتریس «عبارت به سند» 
$\mathbf{A} \in \mathbb{R}^{n \times D}$
در نظر بگیریم. در ابعاد وب این ماتریس در ابعاد 
$n \approx 10^6$
و 
$D \approx 10^10$
بزرگ خواهد شد.
در اینجا عدد 
$(i,j)$
در 
$\mathbf{A}$
تعداد ظهور واژه 
$i$
در سند 
$j$
را نشان می‌دهد.

کارکردن با ماترسی در این ابعاد بزرگ چالش برانگیز است. برای مثال، شاخص 
\lr{LSI}
\LTRfootnote{latent semantic indexing}
\cite{litez58}
و یک مدل موضوعی فراگیر، از 
\lr{SVD}
\LTRfootnote{singular value decomposition}
بر روی ماتریس عبارت به سند استفاده می‌کند. که انجام این عملیات در ابعاد وب قطعا غیرممکن است.

یک مشکل اصلی در قبال مجموعه داده‌های سنگین، حافظه کامپیوتر است. به این دلیل که ابعاد و سرعت حافظه فیزیکی بسیار رشد کمتری در مقایسه با پردازنده‌ها (
\lr{CPU}
) دارد. این پدیده به عنوان دیوار حافظه شناخته می‌شود
\cite{litez139, litez168}
. برای مثال،‌ هر چند ممکن است تمامی رخداد‌های همزمان دوتایی از پیش محاسبه شوند، ولی نگهداری این حجم از داده در حافظه غیر ممکن است. علاوه بر این، گاهی اوقات تخصیص‌‌هایی با بیش از دو عامل هم اهمیت پیدا می‌کنند زیرا درخواست‌ها ممکن است شامل بیش از دو واژه هم باشند. یک راه حل ممکن این است که یک «نمونه» از 
$\mathbf{A}$
 نگهداری شود و همزمانی‌ها بر اساس این نمونه در حین کار تخمین زده شوند. ما حدس می‌زنیم که این روش توسط موتورهای جستجوی امروزی مورد استفاده قرار می‌گیرد، هر چند که روش واقعی قطعا جزو اسرار تجاری آن‌ها است.

هر چند که انتظار می‌رود تخمین‌ها سازگار باشند و فرکانس‌های جفت شده باید با افزایش عبارت به درخواست، کاهش پیدا کنند. 
\autoref{tbl:querylengthcon}
نشان می‌دهد که تخمین‌های بیان شده با موتورهای جستجوی فعلی، همیشه سازگار نیستند.
\begin{table}[h]
\centering
\begin{latin}
\begin{tabular}{lll}
\hhline{===}
Query        				& Hits(Bing)    & Hits(Google) 	\\ \hline
America            			& 150,731,182 	& 393,000,000 	\\
America \& China          		& 15,240,116  	& 66,000,000 	\\
America \& China \& Britain     	& 235,111     	& 6,090,000     \\
America \& CHina \& Britain \& Japan 	& 154,444     	& 23,300,000    \\
\hhline{===}
\end{tabular}
\end{latin}
\caption{
با افزایش تعداد عبارات در درخواست، باید فرکانس‌های جفت شده کاهش پیدا کنند. ولی تخمین‌های بیان شده توسط موتورهای جستجو گاهی این موضوع تثبیت شده را نقض می‌کنند.
}
\label{tbl:querylengthcon}
\end{table}

با اینکه، تعداد کل واژه‌های انگلیسی ( که به‌طور صحیح نوشته شده‌اند) هم اکنون شگفت‌اور است، در بسیاری کاربردهای متن کاوی، ما باید با ابعاد بسیار بزرگتری سر و کار داشته باشیم. در حالی که یک سند ممکن است بیانگر برداری از تک واژه‌ها باشد (به عبارت دیگر، مدل کیسه لغات
\LTRfootnote{bag-of-words}
). معمولا بهتر است سند به عنوانن یک بردار از لغات به صورت 
\lr{l}
پیوسته 
\LTRfootnote{l-shingles}
\cite{litez34}
بیان شود. برای مثال، با استفاده از مدل ۳ پیوسته، جمله‌ی
\lr{"It is a nice day"}
به مجموعه‌ی زیر تجزیه می‌شود. 
\lr{\{"it is a", "is a nice", "a nice day"\}}
این مدل به طور جشمگیری ابعداد داده‌ها را افزایش می‌دهد. به خاطر اینکه، اگر مجموعه‌ی 
$10^6$
تک لغت انگلیسی موجود داشته باشد. مدل ۳ پیوسته تعداد ابعاد را از 
$10^6$
به 
$10^{18}$
افزایش می‌دهد.

\subsection{
چریان‌های داده‌ی حجیم
}
در بسیاری کاربردهای جدید پردازش داده، جریان‌های داده‌ی حجیم نقش بنیادی دارند. جریان‌های داده‌ای که از روترهای اینترنت، سوئیچ‌های تلفن، رصد امسفر، شبکه‌های سنسور، شرایط ترافیکی بزرگراهی، داده‌های مالی و غیره 
\cite{litez5, litez141, litez49, litez19, litez96, litez69, litez91}
حاصل می‌شوند.

برخلاف پایگاه‌ داده‌های سنتی، معمول نیست که جریان‌های داده‌ی حجیم (که با سرعت زیادی منتقل می‌شوند) در جای نگهداری شوند. بنابراین پردازش معمولا به طور همزمان انجام می‌شوند. برای مثال، گاهی اوقات «رصد تصویری» داده‌ها با رصد تغییرات زمانی برخی آماره‌ها کفایت می‌کند. برای مثال آماره‌های نظیر: مجموع، تعداد آیتم‌های مجزا، برخی نرم‌های 
$l_\alpha$
. در برخی کاربردها (برای مثال، طبقه‌بندی صدا/محتوا و جدا سازی) نیاز است یک مدل یادگیری آماری برای کلاسه‌بندی
\LTRfootnote{Classification}
یا خوشه‌بندی
\LTRfootnote{Clustering}
جریان داده‌های حجیم توین شود. ولی معمولا فقط می‌توانیم یک‌بار داده‌ها را مورد بررسی قرار دهیم.

یک خاصیت مهم جریان‌های داده‌ای این است که دینامیک هستند. به عنوان یک مدل محبوب، جریان 
$u$
شامل ورودی‌های 
$(i, u_i)$
است که 
$i = 1 to D$
. برای مثال،
$D = 2^{64}$
زمانی که جریان بیان‌گر 
\lr{IP}
آدرس‌ها است.
\footnote{
هرچند ما بیشتر اوقات تعداد دقیق ابعاد (
\lr{D}
) یک جریان داده را نمی‌دانیم ولی در بیشتر کاربردها کافی است حد بالایی محافظه‌کارانه‌ای را در نظر بگیریم. برای مثال 
$D = 2^{64}$
زمانی که جریان بیانگر 
\lr{IP}
های ورودی است. همچنین این یکی از دلایلی است که داده‌ها بسیار پراکنده هستند. به این نکته توجه داشته باشید که ابعاد بسیار بزرک تاثیری در محاسبه‌ی فاصله‌ها و نمونه‌گیری طی الگوریتم‌های معرفی شده در این پایان‌نامه ندارد.
}
ورودی‌ها ممکن است به هر ترتیبی باشند و ممکن است مرتبا به روز شوند. ذات دینامیک جریان داده‌های حجیم فرآیند نمونه‌گیری را بسیار چالش‌برانگیزتر ا زمانی ‌می‌کند که با داده‌های ایستا سر و کار داریم.

\section{
چالش‌های نمونه‌گیری از داده‌های حجیم
}
در حالی که مسائل جذاب و چالش‌برانگیزی با ورود داده‌های حجیم شکل گرفته‌اند، این پایان‌نامه بر روی توسعه‌ی روش‌های نمونه‌گیری برای محاسبه فاصله در داده‌هایی با ابعاد بسیار بالا با استفاده از حافظه محدود تمرکز دارد.

در کاربردهای مدل‌سازی آماری و یادگیری ماشین، در اغلب موارد به جای داده‌های اصلی به فاصله، به خوصو فاصله‌ی جفتی نیاز داریم. برای مثال، محاسبه ماتریس گرام
\LTRfootnote{Gram matrix}
$\mathbf{AA^T}$
در آمار و یادگیری ماشین معمول است. 
$\mathbf{AA^T}$
بیانگر همه‌ی ضرب‌های داخلی دوتایی در ماتریس داده‌ی 
‌$\mathbf{A}$
است.

دو داده‌ی 
$u_1, u_2 \in \mathbb{R}^D$
داده شده‌اند. ضرب داخلی آن‌ها ( که با 
$a$
نمایش داده می‌شود) و 
$l_\alpha$
(که با 
$d_{(\alpha)}$
نمایش داده می‌شوند با عبارات زیر تعریف می‌شوند:






\section{کاهش بعد}
\section{کاهش بعد}

\section{کاهش بعد}
\subsection{بارگیری مراجع}















